%
% File Name:	report.tex
% Author:	Aditya Ramesh
% Date:		05/18/2013
% Contact:	_@adityaramesh.com
%

\documentclass[11pt,a4paper]{article}

% Bibliography management.
\usepackage{natbib}
\usepackage{hyperref}

% Basic math and figure configuration.
\usepackage[group-separator={,}]{siunitx}
\usepackage{array}
\usepackage{epigraph}
\usepackage{afterpage}
\usepackage{figures}
\usepackage{mathdefs}
\usepackage{geometry}
\counterwithin{table}{section}
\counterwithin{figure}{section}
\numberwithin{equation}{section}

% Font configuration.
\usepackage[
	activate={true,nocompatibility},
	tracking=true
]{microtype}
\usepackage{fontspec}
\usepackage{mathpazo}
\setmainfont[Ligatures = TeX]{TeX Gyre Pagella}
\setmonofont[Ligatures = TeX, Scale = 0.9]{Source Code Pro}
\lstset{basicstyle=\ttfamily}

% Setup for TikZ.
\usetikzlibrary{fit}
\usetikzlibrary{calc}
\makeatletter
\tikzset{
	fitting node/.style={
		inner sep=0pt, fill=none, draw=none, reset transform,
		fit={(\pgf@pathminx,\pgf@pathminy) (\pgf@pathmaxx,\pgf@pathmaxy)}
	},
	reset transform/.code={\pgftransformreset},
	darkstyle/.style={circle, draw=black, fill=white!80!black},
	lightstyle/.style={circle, draw=black, fill=white}
}
\makeatother

% Math definitions.
\newcommand{\weight}[2]{w_{#1 \rightarrow #2}}
\newcommand{\size}{\operatorname{size}}
\newcommand{\capacity}{\operatorname{capacity}}
\newcommand{\complexity}{\operatorname{complexity}}
\newcommand{\apperr}{\mathcal{E}_{\mathrm{app}}}
\newcommand{\esterr}{\mathcal{E}_{\mathrm{est}}}
\newcommand{\opterr}{\mathcal{E}_{\mathrm{opt}}}

%
% This implementation of `widebar` is taken from the following link:
% http://tex.stackexchange.com/questions/16337/can-i-get-a-widebar-without-using-the-mathabx-package.
%
\makeatletter
\let\save@mathaccent\mathaccent
\newcommand*\if@single[3]{%
  \setbox0\hbox{${\mathaccent"0362{#1}}^H$}%
  \setbox2\hbox{${\mathaccent"0362{\kern0pt#1}}^H$}%
  \ifdim\ht0=\ht2 #3\else #2\fi
  }
%The bar will be moved to the right by a half of \macc@kerna, which is computed by amsmath:
\newcommand*\rel@kern[1]{\kern#1\dimexpr\macc@kerna}
%If there's a superscript following the bar, then no negative kern may follow the bar;
%an additional {} makes sure that the superscript is high enough in this case:
\newcommand*\widebar[1]{\@ifnextchar^{{\wide@bar{#1}{0}}}{\wide@bar{#1}{1}}}
%Use a separate algorithm for single symbols:
\newcommand*\wide@bar[2]{\if@single{#1}{\wide@bar@{#1}{#2}{1}}{\wide@bar@{#1}{#2}{2}}}
\newcommand*\wide@bar@[3]{%
  \begingroup
  \def\mathaccent##1##2{%
%Enable nesting of accents:
    \let\mathaccent\save@mathaccent
%If there's more than a single symbol, use the first character instead (see below):
    \if#32 \let\macc@nucleus\first@char \fi
%Determine the italic correction:
    \setbox\z@\hbox{$\macc@style{\macc@nucleus}_{}$}%
    \setbox\tw@\hbox{$\macc@style{\macc@nucleus}{}_{}$}%
    \dimen@\wd\tw@
    \advance\dimen@-\wd\z@
%Now \dimen@ is the italic correction of the symbol.
    \divide\dimen@ 3
    \@tempdima\wd\tw@
    \advance\@tempdima-\scriptspace
%Now \@tempdima is the width of the symbol.
    \divide\@tempdima 10
    \advance\dimen@-\@tempdima
%Now \dimen@ = (italic correction / 3) - (Breite / 10)
    \ifdim\dimen@>\z@ \dimen@0pt\fi
%The bar will be shortened in the case \dimen@<0 !
    \rel@kern{0.6}\kern-\dimen@
    \if#31
      \overline{\rel@kern{-0.6}\kern\dimen@\macc@nucleus\rel@kern{0.4}\kern\dimen@}%
      \advance\dimen@0.4\dimexpr\macc@kerna
%Place the combined final kern (-\dimen@) if it is >0 or if a superscript follows:
      \let\final@kern#2%
      \ifdim\dimen@<\z@ \let\final@kern1\fi
      \if\final@kern1 \kern-\dimen@\fi
    \else
      \overline{\rel@kern{-0.6}\kern\dimen@#1}%
    \fi
  }%
  \macc@depth\@ne
  \let\math@bgroup\@empty \let\math@egroup\macc@set@skewchar
  \mathsurround\z@ \frozen@everymath{\mathgroup\macc@group\relax}%
  \macc@set@skewchar\relax
  \let\mathaccentV\macc@nested@a
%The following initialises \macc@kerna and calls \mathaccent:
  \if#31
    \macc@nested@a\relax111{#1}%
  \else
%If the argument consists of more than one symbol, and if the first token is
%a letter, use that letter for the computations:
    \def\gobble@till@marker##1\endmarker{}%
    \futurelet\first@char\gobble@till@marker#1\endmarker
    \ifcat\noexpand\first@char A\else
      \def\first@char{}%
    \fi
    \macc@nested@a\relax111{\first@char}%
  \fi
  \endgroup
}
\makeatother

\title{Optimization for Neural Networks}
\author{Aditya Ramesh}
\date{}

\begin{document}

\maketitle
\begin{abstract}
Top-scoring submissions for any benchmark involving image recognition or
acoustic modeling invariably use neural networks. The intuition underlying
neural networks is simple. Many sources attempt to convey this intuition, but
lack clarity due to the use of imprecise notation. This makes it difficult for a
practitioner to read one of these sources and subsequently implement neural
networks to solve nontrivial tasks. One of the purposes of this report is to
establish a solid foundation based on current practices, so that the reader can
easily incorporate the results of current research.

Neural networks are prototypical of the large-scale optimization problems often
encountered when applying machine learning in practice. None of the sources
makes the connection between training neural networks and statistical learning
theory. But this connection is necessary in order to explain the differences
between small-scale problems and large-scale problems from the perspective of
optimization. We develop guidelines based on this investigation, and use them to
characterize the most successful optimization algorithms used to train neural
networks today. Finally, we implement several of these algorithms, and discuss
the results based on our current understanding.
\end{abstract}

\section{Introduction to Neural Networks}
\subsection{Representation}
\label{sec:representation}

Neural networks take their inspiration from information processing in biological
systems. In the human brain, information is transmitted between neurons in the
form of electrical and chemical signals sent across synapses. Each neuron
receives signals from a set of input neurons, and, under certain conditions,
will broadcast a signal to a set of output neurons. This response can be thought
of as the result of a local computation involving the input signals. We can
model the flow of information in this network of neurons using a directed graph
$G$, in which the neurons are nodes and the synapses are edges. This rough
biological conceptualization of neural communication leads to a mathematical
structure that we will fashion into a model of computation.

Our goal will now be to derive a mathematical description for~$G$, so that the
resulting neural network can be used for function approximation. To emphasize
that our notion of neural network has little to do with neuroscience, we will
refer to the ``neurons'' in the network as nodes in~$G$. We limit our discussion
to \emph{feed-forward} neural networks, which do not contain cycles.
Consequently,~$G$ must be a directed acyclic graph~(DAG). Now suppose that we
wish to use~$G$ to approximate a function~$f: \reals{m} \to \reals{n}$. If~$f(x)
= y$ for some~$x \in \reals{m}$ and~$y \in \reals{n}$, then our task will be to
use~$G$ to ``learn'' what~$f$ does to transform~$x$ into~$y$.

Let us first impose some organizational structure on~$G$. We delegate to a
set~$I$ of~$m$ input nodes the task of broadcasting the components of~$x$ to
other nodes in~$G$. In order for the neural network to be useful, the
transmission of information must eventually cease at a set~$O$ of~$n$ output
nodes. The information computed by each node in~$O$ will correspond to a
component of~$\hat{y}$, the network's approximation to~$y$. As things stand,
$I$~and~$O$ can consist of arbitrary nodes of~$G$. It would help if we could
hide the entangled mass of nodes of edges (aka~\emph{connections}) involved in
the communication between~$I$ and~$O$, and simply think of~$I$ as~$x$ and~$O$
as~$\hat{y}$.  Fortunately, our existing definitions allow us to do far more
than this.

\begin{figure}
\centering
\begin{minipage}[t]{0.4\linewidth}
	\centering
	\begin{tikzpicture}
	\node[lightstyle, minimum size=20] (a) at (-1, 0) {$A$};
	\node[lightstyle, minimum size=20] (b) at (0,  1) {$B$};
	\node[lightstyle, minimum size=20] (c) at (1,  0) {$C$};
	\node[lightstyle, minimum size=20] (d) at (0, -1) {$D$};
	\draw[-stealth] (a)--(b);
	\draw[-stealth] (a)--(d);
	\draw[-stealth] (b)--(d);
	\draw[-stealth] (c)--(b);
	\draw[-stealth] (c)--(d);
	\end{tikzpicture}
	\subcaption{}\label{fig:simple_dag}
\end{minipage}
\begin{minipage}[t]{0.4\linewidth}
	\centering
	\begin{tikzpicture}
	\node[lightstyle, minimum size=20] (a) at (-1, -1) {$A$};
	\node[lightstyle, minimum size=20] (c) at (1,  -1) {$C$};
	\node[lightstyle, minimum size=20] (b) at (0,  0.6) {$B$};
	\node[lightstyle, minimum size=20] (d) at (0,  2.5) {$D$};
	\draw[-stealth] (a)--(b);
	\draw[-stealth] (a)--(d);
	\draw[-stealth] (b)--(d);
	\draw[-stealth] (c)--(b);
	\draw[-stealth] (c)--(d);
	\end{tikzpicture}
	\subcaption{}\label{fig:simple_layered_dag}
\end{minipage}
\caption{In~\ref{fig:simple_dag}, we see a DAG, and
in~\ref{fig:simple_layered_dag}, the representation of the~DAG as a layered
graph. Notice that the edges~$A \rightarrow D$ and~$C \rightarrow D$ are
manifested as skip connections in the layered
graph.\label{fig:layered_graph_drawing}}
\end{figure}

Any~DAG can be rendered as a layered graph. In a layered graph, nodes are
arranged in horizontal rows, and only vertical connections in one direction,
between nodes in different layers, are allowed. The process by which this is
accomplished is called \emph{layered graph drawing} (see
Figure~\ref{fig:layered_graph_drawing}). Since~$G$ is a~DAG, we can partition
its nodes into an array of successive layers~$L_0, \ldots, L_d$, where~$d$ is
the \emph{depth} of the neural network. The first layer,~$L_0$, consists of the
input nodes ordered from left to right based on the components of~$x$ to which
they correspond. It is called the \emph{input layer}. Not all output nodes
necessarily belong to the last layer, so an \emph{output layer} need not exist
in general. In our case, we assume it does, so~$L_d$ must be the output layer.
Layers in between~$L_0$ and~$L_d$ are called \emph{hidden layers}.
Figure~\ref{fig:two_layer_nn} depicts a small, two-layer neural network.

\begin{figure}
\scriptsize\centering
\begin{tikzpicture}

\def\xdist{1.2}
\def\ydist{1.7}

\node[lightstyle, minimum size=20] (a1) at (-2 * \xdist, 0) {$(0, 1)$};
\node[lightstyle, minimum size=20] (a2) at (-1 * \xdist, 0) {$(0, 2)$};
\node[lightstyle, minimum size=20] (a3) at (0, 0)           {$(0, 3)$};
\node[lightstyle, minimum size=20] (a4) at (1 * \xdist, 0)  {$(0, 4)$};
\node[lightstyle, minimum size=20] (a5) at (2 * \xdist, 0)  {$(0, 5)$};

\node[darkstyle, minimum size=20] (b1) at (-2 * \xdist, \ydist) {$(1, 1)$};
\node[darkstyle, minimum size=20] (b2) at (-1 * \xdist, \ydist) {$(1, 2)$};
\node[darkstyle, minimum size=20] (b3) at (0, \ydist)           {$(1, 3)$};
\node[darkstyle, minimum size=20] (b4) at (1 * \xdist, \ydist)  {$(1, 4)$};
\node[darkstyle, minimum size=20] (b5) at (2 * \xdist, \ydist)  {$(1, 5)$};

\node[lightstyle, minimum size=20] (c1) at (-2 * \xdist, 2 * \ydist) {$(2, 1)$};
\node[lightstyle, minimum size=20] (c2) at (-1 * \xdist, 2 * \ydist) {$(2, 2)$};
\node[lightstyle, minimum size=20] (c3) at (0, 2 * \ydist)           {$(2, 3)$};
\node[lightstyle, minimum size=20] (c4) at (1 * \xdist, 2 * \ydist)  {$(2, 4)$};
\node[lightstyle, minimum size=20] (c5) at (2 * \xdist, 2 * \ydist)  {$(2, 5)$};

\node[below] at (0, -0.6) {$x$};
\node[left] at (-2.5 * \xdist, 0) {\scriptsize Input layer};
\node[left] at (-2.5 * \xdist, 1 * \ydist) {\scriptsize Hidden layer};
\node[left] at (-2.5 * \xdist, 2 * \ydist) {\scriptsize Output layer};
\node[above] at (0, 2 * \ydist + 0.6) {$\hat{y}$};

\draw[-stealth] (a1)--(b1);
\draw[-stealth] (a1)--(b2);
\draw[-stealth] (a2)--(b1);
\draw[-stealth] (a2)--(b2);
\draw[-stealth] (a2)--(b3);
\draw[-stealth] (a3)--(b2);
\draw[-stealth] (a3)--(b3);
\draw[-stealth] (a3)--(b4);
\draw[-stealth] (a4)--(b3);
\draw[-stealth] (a4)--(b4);
\draw[-stealth] (a4)--(b5);
\draw[-stealth] (a5)--(b4);
\draw[-stealth] (a5)--(b5);

\draw[-stealth] (b1)--(c1);
\draw[-stealth] (b1)--(c2);
\draw[-stealth] (b2)--(c1);
\draw[-stealth] (b2)--(c2);
\draw[-stealth] (b2)--(c3);
\draw[-stealth] (b3)--(c2);
\draw[-stealth] (b3)--(c3);
\draw[-stealth] (b3)--(c4);
\draw[-stealth] (b4)--(c3);
\draw[-stealth] (b4)--(c4);
\draw[-stealth] (b4)--(c5);
\draw[-stealth] (b5)--(c4);
\draw[-stealth] (b5)--(c5);
\end{tikzpicture}
\caption{A small, two-layer neural network. The pair $(k, j)$ is used to denote
the $j$th unit in layer $k$.\label{fig:two_layer_nn}}
\end{figure}

Computation in a neural network proceeds from layer to layer. Nodes in a layer
are called \emph{units}, and edges between units (which must necessarily be from
different layers) are called \emph{connections}. As in
Figure~\ref{fig:simple_layered_dag}, connections between units in nonconsecutive
layers can occur; these are called \emph{skip connections}. Consequently, in
order for information to propagate from~$L_k$ to~$L_{k + 1}$, we may require the
outputs of all units from~$L_0$ to~$L_{k - 1}$. Let~$w \coloneqq |L_k|$ denote
the \emph{width} of~$L_k$. Rather than thinking about~$L_k$ as a subgraph
of~$G$, we associate~$L_k$ with a vector~$z_k \in \reals{w}$. The value of
component~$z_{ki}$ is given by the output of the of the~$i$th unit from the left
end of~$L_k$. It is now evident that the vector~$x$ is transformed
into~$\hat{y}$ by a series of successive functions~$\sigma_1, \ldots, \sigma_d$
given by
\begin{align}
\begin{split}
	x \eqqcolon z_0 &\xmapsto{\sigma_1} z_1 \\
	(z_0, z_1) &\xmapsto{\sigma_2} z_2 \\
	(z_0, z_1, z_2) &\xmapsto{\sigma_3} z_3 \\
	\vdots\hspace{0.8cm} &\hphantom{\mapsto{\sigma_3}} \vdots \\
	(z_0, \ldots, z_{d - 1}) &\xmapsto{\sigma_d} z_d \coloneqq \hat{y},
\end{split}
\label{eq:forward_propagation}
\end{align}
where $\sigma_k$ is realized by the units in $L_k$.

\begin{figure}[t]
\scriptsize\centering
\begin{tikzpicture}
\node[lightstyle, minimum size=40] (a) at (-4,2) {$(k - 1, 1)$};
\node[lightstyle, minimum size=40] (b) at (-4,0) {$(k - 1, 2)$};
\node[lightstyle, minimum size=40] (c) at (-4,-2) {$(k - 1, 3)$};
\node[lightstyle, minimum size=40] (d) at (0,0) {$(k, 1)$};
\node[draw, right, align=left] (activation) at (1.5, 0) {
	$\begin{alignedat}{2}
	a_{k1} = \;&\weight{(k - 1, 1)}{k1} z_{k - 1, 1} \;+&& \\
		&\weight{(k - 1, 2)}{k1} z_{k - 1, 2} \;+&& \\
		&\weight{(k - 1, 3)}{k1} z_{k - 1, 3} \;+&&\; b_{k1} \\
	\end{alignedat}$\\[\medskipamount]
	$z_{k1} = u_{k1}(a_{k1})$
};

\draw[-stealth] (a) -- node[sloped, above] {$\weight{(k - 1, 1)}{k1}$} (d);
\draw[-stealth] (b) -- node[sloped, above] {$\weight{(k - 1, 2)}{k1}$} (d);
\draw[-stealth] (c) -- node[sloped, above] {$\weight{(k - 1, 3)}{k1}$} (d);
\end{tikzpicture}

\caption{An illustration how the output of~$(k, 1)$, a unit with three inputs
from~$L_{k - 1}$, is computed. In this case, $\parents(k, 1) = \{(k - 1, 1), (k
- 1, 2), (k - 1, 3)\}$. First, the activation~$a_{k1}$ is computed by forming a
linear combination of the outputs of the units in~$\parents(k, 1)$ with the
corresponding weights to~$(k, j)$, and adding the bias~$b_{k1}$. Then, the
output~$z_{k1}$ is determined by applying the activation function~$u_{k1}$
to~$a_{k1}$.\label{fig:unit_output}}
\end{figure}

The~$j$th unit in the input layer of a neural network simply returns the
corresponding component~$x_j$ of the input vector~$x$. On the other hand, each
unit in the hidden and output layers forms a linear combination of the inputs
from previous layers to which it is connected, and adds a bias parameter to the
result. Suppose that~$L_k$ is not the input layer, so that~$k > 0$. The~$j$th
unit in~$L_k$ is denoted by~$(k, j)$, and its activation~$a_{kj}$ is defined as
\begin{equation}
	a_{kj} \;\coloneqq \sum_{(l, i) \in \parents(k, j)} \weight{li}{kj} z_{li} + b_{kj}.
	\label{eq:activation}
\end{equation}
Here,~$\parents(k, j)$ denotes the parents of~$(k, j)$, which are the units that
have edges directed toward~$(k, j)$. This notation allows us to easily
generalize our discussion to layered networks that incorporate skip connections.
The number~$\weight{li}{kj}$ is the \emph{weight} associated with the
connection~$(l, i) \rightarrow (k, j)$, and $b_{kj}$ the \emph{bias} associated
with $(k, j)$. The quantity~$z_{li}$ is the output of unit~$(l, i)$, and is
obtained by applying the unit's \emph{activation function} $u_{li} : \reals{1}
\to \reals{1}$ to the unit's activation~$a_{li}$:
\begin{equation}
	z_{li} = u_{li}(a_{li}).
	\label{eq:output}
\end{equation}
Figure~\ref{fig:unit_output} shows how the activation of a unit with three
parents is computed.

The process of computing the output vector~$z_k$ corresponds to applying the
function~$\sigma_k$. If~$w \coloneqq |L_k|$, then writing
\[
	(z_0, \ldots, z_{k - 1}) \xmapsto{\sigma_k} z_k
\]
is shorthand for saying that we compute~$z_k$ through~$w$ simultaneous
applications of~(\ref{eq:activation}), followed by~$w$ simultaneous
applications of~(\ref{eq:output}). In order to compute the network's
prediction~$\hat{y} \in \reals{m}$ corresponding to an input~$x \in \reals{n}$,
we apply the functions~$\sigma_1, \ldots, \sigma_d$ as described
by~(\ref{eq:forward_propagation}). This process is called \emph{forward
propagation}.

\subsection{Activation Functions and Feature Learning}
\label{sec:act_func_and_fl}

\begin{figure}[t]
\centering
\begin{subfigure}{0.4\textwidth}
	\centering
	\resizebox{!}{4cm}{%
	\begin{tikzpicture}
	\begin{axis}[
		xmin=-4,
		xmax=4,
		xlabel=$x$,
		ylabel=$y$
	]
	\addplot[mark=none, samples=100] {1.7159 * tanh(2 * x / 3)};
	\end{axis}
	\end{tikzpicture}}
	\caption{Plot of $f(x) \coloneqq a \tanh(b x)$, where $a \coloneqq
	1.7159$ and $b \coloneqq 2/3$.\label{fig:tanh_plot}}
\end{subfigure}\hspace{0.5cm}%
\begin{subfigure}{0.4\textwidth}
	\centering
	\resizebox{!}{4cm}{%
	\begin{tikzpicture}
	\begin{axis}[
		xmin=-4,
		xmax=4,
		xlabel=$x$,
		ylabel=$y$
	]
	\addplot[mark=none, samples=1000] {max(0, x)};
	\end{axis}
	\end{tikzpicture}}
	\caption{Plot of $f(x) \coloneqq \max(0,
	x)$.\newline\label{fig:linear_threshold_plot}}
\end{subfigure} \\
\begin{subfigure}{0.4\textwidth}
	\centering
	\resizebox{!}{4cm}{%
	\begin{tikzpicture}
	\begin{axis}[
		xmin=-4,
		xmax=4,
		xlabel=$x$,
		ylabel=$y$
	]
	\addplot[mark=none, samples=100] {4 * 2/3 * 1.7159 * (cosh(2 * x / 3))^2
	/ (cosh(4 * x / 3) + 1)^2};
	\end{axis}
	\end{tikzpicture}}
	\caption{Plot of the derivative of the function
	in~\ref{fig:tanh_plot}.\label{fig:tanh_der_plot}}
\end{subfigure}\hspace{0.5cm}%
\begin{subfigure}{0.4\textwidth}
	\centering
	\resizebox{!}{4cm}{%
	\begin{tikzpicture}
	\begin{axis}[
		xmin=-4,
		xmax=4,
		xlabel=$x$,
		ylabel=$y$
	]
	\addplot[mark=none, samples=1000] {1/2 * (x/abs(x) + 1)};
	\end{axis}
	\end{tikzpicture}}
	\caption{Plot of the derivative of the function
	in~\ref{fig:linear_threshold_plot}.\label{fig:linear_threshold_der_plot}}
\end{subfigure}
\caption{Plots of two of the most common activation functions and their
derivatives.\label{fig:activation_plots}}
\end{figure}

So far in our discussion, we have avoided making specific choices for the
activation function~$u_{kj} : \reals{1} \to \reals{1}$ associated with unit~$(k,
j)$. Three of the most commonly-used activation functions in the literature are
the identity, scaled $\tanh$, and linear threshold functions (see
Figure~\ref{fig:activation_plots}). (In the neural network literature, linear
threshold functions are called ``rectified linear units''~(ReLU), an instance of
specialized terminology that Mehryar Mohri despises.) Typically, all units in
the same layer are associated with the same activation function. We therefore
drop the redundant second index in the subscript of~$u_{kj}$, and simply refer
to the activation function as~$u_k$.

The choices for the constants~$a$ and~$b$ of the scaled $\tanh$ function are
motivated by several reasons~\citep{lecun-98b}. Firstly, the scaled $\tanh$
function is symmetric and approximately linear about the origin. If the inputs
are decorrelated, our choices for~$a$ and~$b$ cause the variance of the scaled
$\tanh$ function to be close to unity. This accelerates the convergence of
optimization algorithms~(see Section~\ref{sec:pp_and_init}).

Secondly, the choices for these constant helps prevent the \emph{vanishing
gradient problem}. Using these constants causes the second derivative of the
scaled $\tanh$ function to be maximized at~$\pm 1$. This is intentional: $\pm 1$
are the binary target values that are typically used for classification in
practice. Suppose that the asymptotes of the scaled $\tanh$ function had been
at~$\pm 1$ instead of~$\pm a$. Then the weights of the network would drastically
increase, in order to produce the large activation values necessary to drive the
outputs of $\tanh$ function to the target values at its asymptotes.  The
derivative of the scaled $\tanh$ function at these large activation values would
be exponentially small~(see Figure~\ref{fig:tanh_der_plot}). Thus, an
optimization algorithm that used derivative information would be liable to ``get
stuck''.

Finally, the choices for these constants allow us to interpret the output of the
network as a measure of confidence towards its classification. When an input is
near the decision boundary separating instances in two classes, we would like
the network to output a small confidence value to reflect this uncertainty. If
the asymptotes of the scaled $\tanh$ function were at~$\pm 1$, then the large
activation values would force the outputs to one of the two extreme values,
regardless of the certainty of the classification. Our choices for~$a$ and~$b$
avoid this problem.

Unlike the scaled $\tanh$ function, the identity function is typically only used
for regression. One common configuration for regression involves alternating
between layers using the scaled $\tanh$ and identity activation functions. For
classification, we typically only use the $\tanh$ or linear threshold functions.
The adoption of the linear threshold function in the literature is relatively
recent~\citep{nair2010rectified}, but many have found that it drastically
reduces training time and improves generalization when compared to the scaled
$\tanh$ function~\citep{krizhevsky2012imagenet}. Despite these general
guidelines, choosing the best activation functions for a particular problem can
involve a certain degree of experimentation.

One of the principles underlying the effectiveness of neural networks is that of
\emph{hierarchical feature learning}, and is the focus of much of the deep
learning research at~NYU. A \emph{feature} is a ``simple'' quantity derived from
the input that is designed to identify one or more of the input's salient
characteristics. For example, suppose that we wish to identify whether a given
black and white image contains a human face. One potential feature is the
difference between the sum of intensities of the pixels in the left half of the
image and that of the right half of the image. In neural networks, a feature is
a subset of activation values that become large when a given pattern is present
in the input. This is a result of the weights and biases of the network being
attuned to presence the pattern.

The success of neural networks in image recognition and acoustic modeling has
been attributed to their ability to learn a \emph{hierarchy} of features. In
these applications, the location of a layer~$L_k$ in the network determines the
relative scale of the patterns in the input captured by the features. In some
sense, the lowest layers of the network ``zoom in'' to the input to capture
low-level, local details, while the highest layers of the network ``zoom out''
to capture overarching, global patterns (see Figure~\ref{fig:cnn_features}). It
is plausible that this phenomenon can occur, since each~$\sigma_k$ synthesizes
the information from layers~$L_0, \ldots, L_{k - 1}$ to produce $z_k$.

\afterpage{%
	\clearpage
	\thispagestyle{empty}
	\newgeometry{top = 0in, left = 0in, right = 0in}
	\begin{figure}[p]
	\centering
	\resizebox{\textwidth}{!}{%
		\includegraphics[trim = 0in 1in 0in 0in]{graphics/cnn_features.pdf}
	}
	\begin{minipage}[c]{\textwidth - 2in}
	\caption{Visualization of the hierarchy of features in a fully-trained
	convolutional network for image classification, taken
	from~\citet{zeiler2014visualizing}. For each of the first five layers of
	the network, a selection of the highest activation values in the layer
	is shown alongside the corresponding input images. Note that in general,
	visualizing features in a meaningful way is very difficult.
	\label{fig:cnn_features}}
	\end{minipage}
	\end{figure}
	\restoregeometry
	\clearpage
}

The activation functions discussed so far are useful for learning the low- and
mid-level features in the hierarchy. For learning high-level features,
\emph{radial basis functions}~(RBFs) are sometimes more
appropriate~\citep{lecun-98}. A radial basis function~$\phi : \reals{1} \to
\reals{1}$ is a continuous function whose value depends only on the
\emph{radius} from a prescribed center~$c \in \reals{n}$. Suppose that we wish
to approximate a function~$f : \reals{n} \to \reals{1}$ whose values at the
points~$c_1, \ldots, c_m$ are known. By centering an~RBF at each~$c_i$, we can
approximate~$f$ using the function $s : \reals{n} \to \reals{1}$ given by
\begin{equation}
	s(x) \coloneqq \sum_{i = 1}^m \lambda_i \phi(\|x - c_i\|_2),
	\label{eq:rbf_sum}
\end{equation}
where~$\lambda \in \reals{m}$ is an adjustable parameter vector. The most
commonly-used RBFs are the Euclidean and Gaussian basis functions. These
functions are given by
\[
	r \mapsto r \quad\text{and}\quad r \mapsto \exp(-k r^2 / 2),
\]
respectively, where~$k \in \reals{+}$.

Since RBFs are used for learning high-level features, they are usually embedded
into one of the topmost layers of the network. As an example, suppose that we
wish to classify~$32 \times 32$ black and white images of handwritten digits
into ten classes, where each class corresponds to a number from zero to nine.
Suppose that~$L_k$ is a layer with~$32^2$ units. By attaching a ten-unit
layer~$L_{k + 1}$ with RBF activation functions to~$L_k$, we can generate ten
scores from~$z_k \in \reals{32 \times 32}$. The~$i$th score is given by~$(z_{k +
1})_i$, and measures the confidence that the input~$x \in \reals{n}$ belongs to
the~$i$th class.

The idea is to get~$z_k$ to match a \emph{prototype} for one of the classes as
closely as possible. We can view the functions~$\sigma_1, \ldots, \sigma_k$ as
nonlinearly deconstructing and reassembling~$x$ to match one of these
prototypes. The ten prototypes~$c_1, \ldots, c_{10}$, are initialized by running
a clustering algorithm, such as~$k$-means, on the training sample. Each
prototype is treated as the center of an~RBF, and is subsumed into the weights
of the network. The proximity of $z_k$ to the $i$th prototype $c_i$ is given by
$\phi(\|x - c_i\|)$, and is a measure of confidence that $x$ belongs to the
$i$th class.

If~$\phi$ is a \emph{normalized RBF}, then the confidence scores are
probabilities. A normalized RBF~$\phi$ is the normalized form of another basis
function~$\psi$. Hence, the $i$th RBF is given by
\begin{equation}
	\phi(x) \coloneqq \frac{\psi(\|x - c_i\|)}
		{\sum_{j = 1}^{10} \psi(\|x - c_j\|)}.
	\label{eq:normalized_rbf}
\end{equation}
Now the ten outputs,~$\phi(\|x - c_i\|)$, $i \in [1, 10]$, form a discrete
probability distribution over the classes.

\subsection{Classification and Regression}

We have seen that a neural network is a biologically-inspired nonlinear function
controlled by two adjustable vectors of parameters: the weights $w$ and the
biases~$b$. Now consider a sample~$S \coloneqq \{(x_1, y_1), \ldots, (x_s,
y_s)\}$, where each~$x_k \in \reals{n}$ is an input vector, and each~$y_k$ is
the target vector that we aim to predict when we are given~$x_k$. We focus on
two categories of tasks that we can perform using neural networks:
classification and regression. For regression, we have~$y_k \in \reals{m}$, so
the output layer consists of a single unit.

For classification, two encoding schemes are possible. The first scheme can only
be used for \emph{unary classification}, in which we wish to classify~$x_k$ into
one of several mutually-exclusive classes. In this scheme, each class is
associated with an index, so~$y_k, \hat{y}_k \in \integers{1}$. These values are
called a \emph{place codes}. A special case of unary classification is
\emph{binary classification}, in which we seek to place~$x_k$ into one of two
mutually-exclusive classes. When using place codes, the output layer consists of
a single unit.

The second scheme can be used for unary classification as well as
\emph{multiclass classification}, in which the~$x_k$ can belong to one or more
classes. This time,~$y_k \in \{0, 1\}^m$ and $\hat{y}_k \in \reals{m}$. These
values are called \emph{distributed codes}. The component~$(y_k)_i$ is one
if~$x_k$ belongs to the $i$th class, and zero otherwise. On the other hand,
$(\hat{y}_k)_i$ is a measure of confidence that $x_k$ belongs to the $i$th
class. Whether a larger value indicate increased or decreased confidence varies
based on convention.

Distributed codes possess the advantage that they scale well to large
numbers of classes, while place codes do not~\citep{lecun-98b}. When using place
codes, the single output unit of the network must assume one of finitely many
values. If a large number of classes occur with nontrivial probabilities, then
each unit in the penultimate layer of the network must generate an output close
to zero almost all of the time. This becomes increasingly difficult as the
number of classes grows.

\subsection{The Loss Function}

\begin{figure}
\scriptsize\centering
\begin{tikzpicture}

\def\xdist{1.2}
\def\ydist{1.7}

\node[lightstyle, minimum size=20] (a1) at (-2 * \xdist, 0) {$(0, 1)$};
\node[lightstyle, minimum size=20] (a2) at (-1 * \xdist, 0) {$(0, 2)$};
\node[lightstyle, minimum size=20] (a3) at (0, 0)           {$(0, 3)$};
\node[lightstyle, minimum size=20] (a4) at (1 * \xdist, 0)  {$(0, 4)$};
\node[lightstyle, minimum size=20] (a5) at (2 * \xdist, 0)  {$(0, 5)$};

\node[darkstyle, minimum size=20] (b1) at (-2 * \xdist, \ydist) {$(1, 1)$};
\node[darkstyle, minimum size=20] (b2) at (-1 * \xdist, \ydist) {$(1, 2)$};
\node[darkstyle, minimum size=20] (b3) at (0, \ydist)           {$(1, 3)$};
\node[darkstyle, minimum size=20] (b4) at (1 * \xdist, \ydist)  {$(1, 4)$};
\node[darkstyle, minimum size=20] (b5) at (2 * \xdist, \ydist)  {$(1, 5)$};

\node[lightstyle, minimum size=20] (c1) at (-2 * \xdist, 2 * \ydist) {$(2, 1)$};
\node[lightstyle, minimum size=20] (c2) at (-1 * \xdist, 2 * \ydist) {$(2, 2)$};
\node[lightstyle, minimum size=20] (c3) at (0, 2 * \ydist)           {$(2, 3)$};
\node[lightstyle, minimum size=20] (c4) at (1 * \xdist, 2 * \ydist)  {$(2, 4)$};
\node[lightstyle, minimum size=20] (c5) at (2 * \xdist, 2 * \ydist)  {$(2, 5)$};

\node[lightstyle, minimum size=27] (target) at (5, 0) {$y_k$};

\draw (-2.85, 4.8) rectangle (5.5, 5.8) node[fitting node] (loss) {};
\draw (1.325, 5.3) node {Loss};

\node[below] at (0, -0.6) {$x$};
\node[left] at (-2.5 * \xdist, 0) {\scriptsize Input layer};
\node[left] at (-2.5 * \xdist, 1 * \ydist) {\scriptsize Hidden layer};
\node[left] at (-2.5 * \xdist, 2 * \ydist) {\scriptsize Output layer};
%\node[above] at (0, 2 * \ydist + 0.6) {$\hat{y}$};

\draw[-stealth] (a1)--(b1);
\draw[-stealth] (a1)--(b2);
\draw[-stealth] (a2)--(b1);
\draw[-stealth] (a2)--(b2);
\draw[-stealth] (a2)--(b3);
\draw[-stealth] (a3)--(b2);
\draw[-stealth] (a3)--(b3);
\draw[-stealth] (a3)--(b4);
\draw[-stealth] (a4)--(b3);
\draw[-stealth] (a4)--(b4);
\draw[-stealth] (a4)--(b5);
\draw[-stealth] (a5)--(b4);
\draw[-stealth] (a5)--(b5);

\draw[-stealth] (b1)--(c1);
\draw[-stealth] (b1)--(c2);
\draw[-stealth] (b2)--(c1);
\draw[-stealth] (b2)--(c2);
\draw[-stealth] (b2)--(c3);
\draw[-stealth] (b3)--(c2);
\draw[-stealth] (b3)--(c3);
\draw[-stealth] (b3)--(c4);
\draw[-stealth] (b4)--(c3);
\draw[-stealth] (b4)--(c4);
\draw[-stealth] (b4)--(c5);
\draw[-stealth] (b5)--(c4);
\draw[-stealth] (b5)--(c5);

\path
	let
		\p1 = (c1),
		\p2 = (c2),
		\p3 = (c3),
		\p4 = (c4),
		\p5 = (c5),
		\p6 = (target),
		\p7 = (loss.south)
	in
		node (d1) at (\x1, \y7 + 3) {}
		node (d2) at (\x2, \y7 + 3) {}
		node (d3) at (\x3, \y7 + 3) {}
		node (d4) at (\x4, \y7 + 3) {}
		node (d5) at (\x5, \y7 + 3) {}
		node (d6) at (\x6, \y7 + 3) {};

\draw[-stealth] (c1)--(d1);
\draw[-stealth] (c2)--(d2);
\draw[-stealth] (c3)--(d3);
\draw[-stealth] (c4)--(d4);
\draw[-stealth] (c5)--(d5);
\draw[-stealth] (target)--(d6);
\end{tikzpicture}
\caption{A visualization of how the $k$th instance~$(x_k, y_k)$ is fed into the
two-layer neural network from
Figure~\ref{fig:two_layer_nn}.\label{fig:nn_with_loss}}
\end{figure}

Our goal is now to formulate the objective for the optimization algorithm used
to calibrate the weights and biases. To do this, we need a function that
measures how well the network's prediction~$\hat{y}_k \in \reals{m}$ for $x_k
\in \reals{n}$ matches the target value~$y_k \in \reals{m}$. This function is
called the \emph{loss function} or \emph{cost function} (see
Figure~\ref{fig:nn_with_loss}). The choice of the loss function depends on the
type of problem we would like to solve.

One way to arrive at the loss function is to assign a probabilistic
interpretation to the network outputs~\citep{ml_bishop}. Let~$w$ and~$b$ be the
vectors of weights and biases of the network. By choosing an explicit
representation for the conditional distribution~$p(y_k \given x_k, w, b)$, we
are led to a canonical loss function corresponding to this choice. Let~$X =
\{x_1, \ldots, x_s\}$ and~$Y = \{y_1, \ldots, y_s\}$. If we assume that the
instances~$(x_k, y_k) \in S$ are iid, then we can apply the chain rule to write
\[
	p(Y \given X, w, b) = \prod_{k = 1}^s p(y_k \given x_k, w, b).
	\label{eq:likelihood_function}
\]
The LHS of~(\ref{eq:likelihood_function}) is called the likelihood function. The
canonical loss function is obtained by maximizing the likelihood function over
the~$w$ and~$b$. Maximizing the likelihood function is equivalent to minimizing
its negative logarithm, which is given by
\begin{equation}
	-\ln{p(Y \given X, w, b)} = -\sum_{k = 1}^s \ln{p(y_k \given x_k, w, b)}.
	\label{eq:nll}
\end{equation}
The LHS of~(\ref{eq:nll}) is called the negative log-likelihood function~(NLL).

For regression, it is often appropriate to assume that
\[
	p(y_k \given x_k, w, b) = N(y_k \given \hat{y}_k, \beta^{-1}),
\]
where~$N$ is the normal distribution, which for mean~$\mu$ and
variance~$\sigma^2$ is defined as
\[
	N(x \given \mu, \sigma^2) \coloneqq \frac{1}{\sqrt{2 \pi \sigma^2}}
		\exp\left( -\frac{1}{2 \sigma^2} (x - \mu)^2 \right).
\]
The value~$\hat{y}_k$ is the network's prediction for~$x_k$, and~$\beta$ is the
precision (inverse variance) of the Gaussian noise. The corresponding NLL
function is given by
\[
	\frac{\beta}{2} \sum_{k = 1}^s \left(\hat{y}_k - y_k\right)^2 -
		\frac{s}{2} \ln\beta + \frac{s}{2} \ln(2 \pi).
\]
We discard the constant terms, which do not affect the minimization process.
This leaves us with the sum-of-squares loss function, which is given by
\begin{equation}
	E(w, b)
	\coloneqq \frac{1}{2} \sum_{k = 1}^s \left(\hat{y}_k - y_k\right)^2
	= \sum_{k = 1}^s e(\hat{y}_k, y_k),
	\label{eq:sum_of_squares}
\end{equation}
where~$e$ is the per-instance loss function
\[
	e(\hat{y}_k, y_k) \coloneqq \frac{1}{2} \left(\hat{y}_k - y_k\right)^2.
\]

The sum-of-squares error function is generally inappropriate for classification.
We adopt the use of distributed codes, so~$y_k \in \{0, 1\}^m$ and $\hat{y}_k
\in \reals{m}$. We further suppose that the components of~$\hat{y}_k$ determine
a discrete probability distribution over the classes. For convenience, we
define~$y_{ki} \coloneqq (y_k)_i$ and~$\hat{y}_{ki} \coloneqq (\hat{y}_k)_i$.
Our assumption is that the conditional distribution is categorical, and given by
\[
	p(y_k \given x_k, w, b) = \prod_{i = 1}^m \left(\hat{y}_{ki}\right)^{y_{ki}}.
\]
The corresponding NLL function is the \emph{multiclass cross-entropy function},
and does not have any constant terms that we can discard. So we define
\[
	E(w, b)
	\coloneqq -\sum_{k = 1}^s \sum_{i = 1}^m y_{ki} \ln\hat{y}_{ki}
	= \sum_{k = 1}^s e(\hat{y}_k, y_k),
\]
where~$e$ is the per-instance loss function
\[
	e(\hat{y}_k, y_k) \coloneqq -\sum_{i = 1}^m y_{ki} \ln\hat{y}_{ki}.
\]
In the special case where we are performing binary classification, with~$y_k \in
\{0, 1\}$ and~$\hat{y}_k \in \reals{1}$, we can write
\[
	p(y_k \given x_k, w, b) = \left(\hat{y}_k\right)^{y_k}
		\left(1 - \hat{y}_k\right)^{1 - y_k}
\]
and
\[
	e(\hat{y}_k, y_k) \coloneqq -\big(
		y_k \ln\hat{y}_k + (1 - y_k) \ln(1 - \hat{y}_k) \big).
\]

The cross-entropy loss function is almost exclusively used with a special
activation function for the output layer of the network. This activation
function is the \emph{softmax} function; for~$z \in \reals{m}$, it is given by
\begin{equation}
	\phi(z_i) \coloneqq \frac{\exp(z_i)}{\sum_{j = 1}^m \exp(z_j)}.
	\label{eq:softmax}
\end{equation}
This is a normalized RBF of the form given by~(\ref{eq:normalized_rbf}). It
follows that using~(\ref{eq:softmax}) as the output activation function will
guarantee that the components of~$\hat{y}_k$ determine a discrete probability
distribution over the classes. For binary classification, (\ref{eq:softmax})
reduces to the \emph{logistic sigmoid} function upon setting the
component~$\hat{y}_{k2}$, which corresponds to the second class, to zero:
\begin{align*}
	\phi(\hat{y}_1)
	&= \frac{\exp(\hat{y}_1)}{\exp(\hat{y}_1) + \exp(\hat{y}_2)} \\
	&= \frac{\exp(\hat{y}_1)}{\exp(\hat{y}_1) + 1} \\
	&= \frac{1}{1 + \exp(-\hat{y}_1)}.
\end{align*}
Thus, the softmax function can be viewed as a generalization of the logistic
sigmoid function to more than two classes.

\subsection{Data Preprocessing and Initialization}
\label{sec:pp_and_init}

Preprocessing the data can have drastic effects on convergence of the
optimization algorithm used for learning. However, normalizing the features (the
components of an input~$x_k \in \reals{n}$) must be done with great care, so as
not to destroy useful information encoded by the relative magnitudes of a
feature. Suppose that it is equally important that we predict the target value
for any input~$x_k$ from our training sample~$D$. This is \emph{not} the case,
for instance, when we wish to predict whether the person corresponding to an
input~$x_k$ has a rare disease. Then it can be beneficial to center and scale
the features by subtracting the mean vector~$\mu$ and scaling by the variance
vector~$\sigma$. Specifically, each input~$x_k$ is normalized by computing
\begin{equation}
	\bar{x}_k \coloneqq \frac{x_k - \mu}{\sigma}.
	\label{eq:center_and_scale}
\end{equation}

In general, it is recommended to \emph{decorrelate} the features whenever
possible~\citep{lecun-98b}. Doing so decouples the updates to the individual
weights of the network during the optimization process, which accelerates
convergence. \citet{lecun-98b} give an intuitive explanation for why
decorrelating the inputs can be beneficial. Consider the extreme case where all
components of the inputs are positive. Let~$(1, j)$ be a unit in the second
layer of the network, and let~$\tilde{x}_k$ consist of the components of~$x_k$
corresponding to~$\parents{(1, j)}$ (see Figure~\ref{fig:nn_with_loss}).
Let~$\delta$ be the scalar error computed for $(1, j)$ using the techniques
discussed in Section~\ref{sec:computing_derivatives}. Then a na\"{i}ve
first-order optimization algorithm will update the weights to~$(1, j)$ by an
amount proportional to~$\delta \tilde{x}_k$. Thus, the weights to~$(1, j)$ will
either \emph{all} increase or \emph{all} decrease (depending on~$\sign(\delta)$)
after each update. This makes finding a good weight configuration difficult. A
more sophisticated approach to decorrelating the inputs involves
PCA~\citep{lecun-98b, krizhevsky2012imagenet}.

The initial values of the weights can have a significant impact on both
convergence speed and generalization error. Suppose that we use the scaled
$\tanh$ activation function. In order to avoid vanishing gradient problem, we
would like the activation values to remain in the range~$[-1, 1]$, which is
within the linear region of the scaled $\tanh$ function. One way of enforcing
this is to require that the activation values have zero mean and unit
variance~\citep{lecun-98b}.

Suppose that the inputs are centered and scaled according
to~(\ref{eq:center_and_scale}), so that
\[
	\E(x_i) = 0 \quad\text{and}\quad \Var(x_i) = 1,
\]
for each~$i \in [1, n]$. From~(\ref{eq:activation}), we know that the activation
of~$(1, j)$ is given by
\[
	a_{1j} \;\coloneqq \sum_{(0, i) \in \parents(1, j)} \weight{0l}{1j} x_{0l} + b_{1j}.
\]
If the weights are chosen independently from the inputs, and both the weights
and the biases have zero mean, then we have
\begin{align*}
	\E(a_{1j})
	&= \E\Bigg(\sum_{(0, i) \in \parents(1, j)} \weight{0l}{1j} x_{0l} + b_{1j}\Bigg) \\
	&= \sum_{(0, i) \in \parents(1, j)} \E(\weight{0l}{1j} x_{0l} + b_{1j}) \\
	&= \sum_{(0, i) \in \parents(1, j)} \left(\E(\weight{0l}{1j} x_{0l}) + \E(b_{1j})\right) \\
	&= \sum_{(0, i) \in \parents(1, j)} \E(\weight{0l}{1j}) \E(x_{0l}) = 0.
\end{align*}
Thus, our requirement that the activation values have zero mean is satisfied.

We now describe how to enforce that~$\Var(a_{1j}) = 1$. Let $f \coloneqq
|\text{Pa}(1, j)|$, and suppose that all weights have the same standard
deviation~$\sigma$. The value~$f$ is called the \emph{fan-in} of~$(1, j)$. To
solve for the value of~$\sigma$ that forces~$\Var(a_{1j}) = 1$, we compute
\begin{align*}
	\Var(a_{1j})
	&= \Var\Bigg(\sum_{(0, i) \in \parents(1, j)} \weight{0l}{1j} x_{0l} + b_{1j}\Bigg) \\
	&= \Var\Bigg(
		\sum_{(0, i) \in \parents(1, j)} \weight{0l}{1j} x_{0l} +
		\sum_{(0, i) \in \parents(1, j)} b_{1j}
	\Bigg) \\
	&= \Var\Bigg(\sum_{(0, i) \in \parents(1, j)} \weight{0l}{1j} x_{0l}\Bigg)\\
	&= \sum_{(0, i) \in \parents(1, j)} \left(
		\Var(\weight{0l}{1j}) \Var(x_{0l}) -
		\E(\weight{0l}{1j})^2 \E(x_{0l})^2
	\right) \\
	&= \sum_{(0, i) \in \parents(1, j)} \Var(\weight{0l}{1j}) \Var(x_{0l}) \\
	&= \sum_{(0, i) \in \parents(1, j)} \sigma^2 = f \sigma^2.
\end{align*}
So in order to enforce that $\Var(a_{1j}) = 1$, we should sample the weights
from a distribution with zero mean and standard deviation given by
\begin{equation}
	\sigma = \frac{1}{\sqrt{f}}.
	\label{eq:weight_stddev}
\end{equation}
The most common choices of distribution for weight initialization are the
uniform and normal distributions~\citep{lecun-98b, krizhevsky2012imagenet}.

The training dynamics of neural networks are vitally important but poorly
understood. As the weights and biases of the network are calibrated using the
optimization algorithm, the distributions of the activation values at each layer
evolve. For deep networks with many layers, the vanishing gradient problem is
greatly compounded. In particular, the greater the depth of a layer in the
network, the greater the tendency of the activation values to cluster around
zero or become very large~\citep{glorot2010understanding}. This causes the
vanishing gradient problem discussed in
Section~\ref{sec:act_func_and_fl}. The subtle numerical issues
resulting from these complex interactions largely determines whether, when, and
how a given optimization algorithm will converge.

The use of a good weight initialization scheme can help mitigate the undesirable
saturation of the activation values~\citep{glorot2010understanding}. Several
authors have recently proposed new initialization schemes that they found were
more effective than the one given here~\citep{glorot2010understanding,
martens2010deep}. Another strategy is to incorporate a form of internal
normalization into the network by adding \emph{local response normalization}
layers~\citep{krizhevsky2012imagenet}. Part of the reason that training deep
neural networks is so difficult is that these dynamics are so poorly understood.
Development of a deeper understanding of what is happening in these situations
is likely to have widespread practical consequences.

\subsection{Computing Derivatives}
\label{sec:computing_derivatives}

All of the optimization algorithms that we will discuss require derivative
information. We establish some useful notation before proceeding. Let~$e$ be the
per-instance loss function, and let~$\theta \coloneqq (w, b) \in \reals{p}$ be
the vector consisting of the weights and biases of the network. We now
fix~$(x_k, y_k) \in S$ and~$\bar{\theta} \in \reals{p}$, and define~$\varphi :
\theta \mapsto e(\hat{y}_k, y_k)$, where~$\hat{y}_k$ is the prediction of the
network with parameters~$\bar{\theta}$ for~$x_k$. The gradient vector and
Hessian matrix of~$\varphi$ evaluated at~$\bar{\theta}$ are given by
\[
	\bar{g} \coloneqq (\nabla_\theta \varphi)(\bar{\theta})
	\quad\text{and}\quad
	\widebar{H} \coloneqq (\nabla^2_\theta \varphi)(\bar{\theta}),
\]
respectively. How can we compute~$\bar{g}$ and~$\widebar{H}$ efficiently?

In Section~\ref{sec:representation}, we introduced the forward propagation
procedure. Given an input~$x \in \reals{n}$, we evaluate the
functions~$\sigma_1, \ldots, \sigma_d$ corresponding to each layer in
succession, culminating in the prediction~$\hat{y} \in \reals{m}$~(see
Figure~\ref{fig:nn_with_loss}). Each forward propagation requires~$O(p)$ steps.
An obvious way to compute the gradient is to use finite differences. For each~$i
\in [1, p]$, we calculate
\[
	(\bar{g})_i \approx \frac{\varphi(\bar{\theta} + \epsilon e_i) -
		\varphi(\bar{\theta})}{\epsilon},
\]
where~$\epsilon \ll 1$ is chosen to be small enough so that the difference is
accurate, but not \emph{so} small that the numerator only contains garbage after
subtraction. This method is rather inefficient. Each component of~$\bar{g}$
requires a finite difference, and each finite difference requires an additional
forward propagation. This results in a total of~$O(p^2)$ steps to compute the
gradient. Using central differences for additional accuracy doubles the amount
of work.

Fortunately, $\bar{g}$~can be computed in only~$O(p)$ steps. We first compute
the derivatives of~$u_d$~with respect to the parameters and activation values of
the units in~$L_d$. Using the chain rule, we can compute the derivatives
of~$\varphi$ with respect to the parameters of~$L_d$. The components
of~$\bar{g}$ corresponding to these parameters are now determined. To compute
the same quantities for~$u_{d - 1}$, we require the derivatives of~$u_{d}$.
Proceeding with the computations now gives us the information necessary to
compute the derivatives of~$\varphi$ with respect to the parameters of~$L_{d -
1}$. The components of~$\bar{g}$ corresponding to these parameters are now
determined. Repeating this process allows us to accumulate all of the components
of~$\bar{g}$ by visiting the layers of the network in \emph{reverse order}.
This procedure is hence called~\emph{backpropagation}; details can be found
in~\citet{ml_bishop}. Backpropagation can be tricky to implement, so finite
differences are often used to validate correctness.

Can we extend backpropagation to compute~$\widebar{H}$? Yes, but there is a problem.
A small neural network has on the order of $10^6$~parameters. Hence,
$\bar{g}$~has $10^6$~elements, and~$\widebar{H}$ has $10^{12}$~elements. So even for
a small network, computing~$\widebar{H}$ is completely intractable. However,
both~$\diag(\widebar{H})$ and~$\widebar{H}v$ for a vector~$v \in \reals{p}$ can be
computed efficiently using modified versions of
backpropagation~\citep{ml_bishop}. Both of these quantities are exploited by
various optimization algorithms.

\subsection{Convolutional Networks}

Convolutional networks are responsible for the success of models based on
learning hierarchical representations~(see Section~\ref{sec:act_func_and_fl}).
The area of machine learning that studies such models is called ``deep
learning''. Of all of the methods commonly studied in deep learning,
convolutional networks are the most practical: the top scoring methods for any
benchmark involving image recognition or acoustic modeling invariably use them.
Forward propagation's low computational cost (on the order of a few milliseconds
per evaluation) allows convolutional networks to be used in low-power devices
such as cameras and robots. Training convolutional networks using
backpropagation has been studied extensively in the
literature~\citep{lecun-98b}. Due to space and time constraints, we do not
discuss them here; see~\citet{ufldl-website} for an introduction.

\section{Concepts from Statistical Learning Theory}
\label{src:concepts_from_slt}

\subsection{Generalization and PAC-learning}

Our initial goal when motivating the discussion for neural networks was to
``learn'' what a given function~$f: \reals{n} \to \reals{m}$ does to
transform~$x \in \reals{n}$ to~$y \in \reals{m}$. But what exactly does it mean
to learn? So far, we have neglected to provide a definition for what it means
for an algorithm ``learn'' a model to solve a given problem. A rigorous
definition for learning is given in the field of statistical learning theory.
This definition will have widespread consequences in shaping our criteria used
to characterize optimization algorithms.

When performing binary classification, each training instance~$(x_k, y_k) \in S$
is implicitly associated with a target \emph{concept} that we wish to learn.
This concept is a mapping~$c$ from the input space~$X$ to the output space~$Y
\coloneqq \{0, 1\}$, such that~$y_k = c(x_k)$. The set of all concepts
associated with our classification task is called the \emph{concept class}, and
is denoted by~$C$. A concept can be something simple, such as the set of
axis-aligned rectangles in~$\reals{2}$, or something exceedingly complex, such
as the manifold of human faces in~$\reals{256 \times 256}$, the space of~$256
\times 256$, 1-bit, black and white images. We assume that all instances are
drawn iid from some unknown distribution~$D$~over~$X$.

Suppose that we are given a family of candidate functions~$F$, such that~$f : X
\to \{0, 1\}$ for every~$f \in F$. The task of our learning algorithm is to
minimize the probability of a misclassification from an instance~$x \sim D$.
This leads to the following definition for \emph{generalization error}.

\begin{definition}[Generalization error~\citep{ml_mohri}]
Given a candidate function~$f \in F$, a target concept~$c \in C$, and an
underlying distribution~$D$, the \emph{generalization error} or \emph{risk}
of~$f$ is defined as
\[
	R(f) \coloneqq \Pr_{x \sim D}(f(x) \neq c(x)) =
		\E_{x \sim D} \left(\ind{f(x) \neq c(x)}\right).
\]
\end{definition}

The generalization error is not a quantity that we can directly measure.
Instead, we must work with the \emph{empirical error}, the average number of
misclassifications over our training sample~$S \sim D$.

\begin{definition}[Empirical error~\citep{ml_mohri}]
Given a candidate function~$f \in F$, a target concept~$c \in C$, and a
sample~$S \coloneqq \{(x_1, y_k), \ldots, (x_s, y_s)\} \sim D$, the
\emph{empirical error} or \emph{empirical risk} of~$f$ is defined as
\[
	\hat{R}(f) \coloneqq \frac{1}{s} \sum_{i = 1}^s \ind{f(x) \neq c(x)}.
\]
\end{definition}

There is a simple relationship between the empirical and generalization errors.
Let~$f \in F$. Using the linearity of expectations and the fact that~$S$ is
drawn iid from~$D$, it is easy to show that
\[
	\E_{S \sim D} (\hat{R}(f)) = R(f).
\]
In other words, the generalization error of~$f$ is simply the average
misclassification rate over samples drawn from~$D$. We are now ready to present
the definition of probably approximate correct~(PAC) learning.

\begin{definition}[PAC-learning~\citep{ml_mohri}]
Suppose that we are performing binary classification of inputs from an input
space~$X$, and let~$C$ be the associated concept class. Let~$O(n)$ be an upper
bound on the space complexity of an input~$x \in X$. Given an algorithm~$A$ and
a sample~$S \sim D$, let~$f_S \in F$ denote the candidate function found by~$A$
for~$S$. Then~$C$ is PAC-learnable if there exists an algorithm~$A$ and a
polynomial function~$g : \reals{4} \to \reals{1}$ such that for any~$\epsilon,
\delta \in (0, 1]$, for all distributions~$D$~over~$X$, and for any target
concept~$c \in C$, the following holds for any sample~$S \sim D$ of size~$s \geq
g(1/\epsilon, 1/\delta, n, \size(c))$:
\[
	\Pr_{S \sim D} (R(f_S) \leq \epsilon) \geq 1 - \delta.
\]
If $A$ runs in~$g(1/\epsilon, 1/\delta, n, \size(c))$, then~$C$ is said to be
efficiently PAC-learnable. In this case, $A$~is called a PAC-learning algorithm
for~$C$.
\end{definition}

The definition for PAC-learning is verbose, but the underlying intuition is
simple. Suppose that we have a target generalization error~$\epsilon$ that we
wish to attain with probability~$1 - \delta$ on any sample~$S \sim D$. Then~$C$
is PAC-learnable if the sample size required to meet our goals is bounded by a
polynomial function of~$1/\epsilon$ and~$1/\delta$. Roughly speaking, $A$ must
find a function in~$F$ that is \emph{approximately correct} with \emph{high
probability}. But it must be able to do this using a sample size that is not
``too large''. The analogous notion of space and time complexity for sample size
requires this ``high probability'' bound.

One of the goals of statistical learning theory is to derive upper bounds on the
generalization error in various situations. When such an upper bound applies to
the particular situation at hand, we know that it is possible, at least in
principle, to learn efficiently. Many of these upper bounds take the following
form. Let~$\delta \in (0, 1]$ be our target confidence, and let~$s$ be the size
of the training samples drawn from~$D$. Then with probability~$1 - \delta$, it
holds for any~$f \in F$ that
\begin{equation}
	R(f) \leq \hat{R}(f) + c \sqrt{\frac{\capacity(F)}{s}},
	\label{eq:generalization_bound}
\end{equation}
where~$c \in \reals{+}$ and~$\capacity(F)$ is a quantity that measures the
complexity of our chosen family of functions. When we
apply~(\ref{eq:generalization_bound}), we often subsume auxiliary constants
into~$c$. So two occurrences of~$c$ do \emph{not} necessary represent the same
constant value.

\subsection{ERM vs SRM}
\label{sec:erm_vs_srm}

\begin{figure}
\centering
\resizebox{0.6 \textwidth}{!}{%
	\includegraphics{graphics/srm_graph.pdf}
}
\caption{Visualization of the SRM procedure (adapted from~\citet{ml_mohri}). The
SRM procedure minimizes an upper bound of the generalization error that is a sum
of the training error and the complexity term.\label{fig:srm_graph}}
\end{figure}

The central idea of statistical learning theory is that learning is a
\emph{tradeoff} between minimizing the training error, and the complexity of the
family of functions used to model~$C$~(see Figure~\ref{fig:srm_graph}). The
former and latter quantities correspond to the first and second terms on the RHS
of~(\ref{eq:generalization_bound}), respectively. A particularly illuminating
way of expressing this tradeoff is as follows. Let
\[
	f^\star \coloneqq \argmin_{f} R(f) \quad\text{and}\quad
	f_F^\star \coloneqq \argmin_{f \in F} R(f).
\]
In other words, $R(f^\star)$~is the best we can ever hope to do using an
\emph{arbitrary} function, while $R(f_F^\star)$~is the best we can do using a
\emph{fixed} function family $F$. Given a function~$f \in F$, we define the
\emph{excess error}~$\mathcal{E}$ as
\begin{equation}
	\mathcal{E} \coloneqq R(f) - R(f^\star)
		= (R(f_F^\star) - R(f^\star)) + (R(f) - R(f_F^\star))
		= \apperr + \esterr,
	\label{eq:app_est_decomp}
\end{equation}
where
\[
	\apperr = R(f_F^\star) - R(f^\star)
	\quad\text{and}\quad
	\esterr = R(f) - R(f_F^\star)
\]
The quantity~$\apperr$ is called the \emph{approximation error}, and measures
how well functions in~$F$ can approximate functions in~$C$. It is generally
intractable, as we have no way of estimating it. The quantity~$\esterr$ is
called the \emph{estimation error}, and measures the performance of~$f$ relative
to that of the best-in-class hypothesis~$f_F^\star$. It is easy to show
that~$\esterr$ is bounded above by the second term on the~RHS
of~(\ref{eq:generalization_bound}). Equation~\ref{eq:app_est_decomp} tells us
that the quality of a candidate function~$f$ is determined by the sum of these
errors.

Reducing the training error~$\hat{R}(f)$ decreases $\esterr$, and
\emph{increasing} $\capacity(F)$ decreases $\apperr$. Suppose we are given a
fixed training sample~$S$, and wish to reduce $\apperr$. This requires
increasing $\capacity(F)$, which enlarges the search space of functions for the
optimization algorithm. Since the optimization algorithm must now look in a
strictly larger space of functions, $\esterr$ is likely to increase. Let us try
reduce~$\esterr$. Since~$S$ is fixed, we cannot get more data; our only choice
is to reduce $\capacity(F)$, which is likely to increase $\apperr$. We cannot
continue to decrease one of the two errors without beginning to increasing the
other~(see Figure~\ref{fig:srm_graph}); this is the
\emph{approximation-estimation tradeoff}.

Minimizing only~$\hat{R}(f)$ while ignoring $\capacity(F)$ is called
\emph{empirical risk minimization}~(ERM), and is inadvisable on both theoretical
and practical grounds. An alternative procedure that controls $\capacity(F)$ is
called \emph{structural risk minimization}~(SRM). It works as follows. Let~$c
\in \reals{+}$, and define
\begin{equation}
	F_\lambda \coloneqq \{ f \in F : \complexity(f) \leq c \},
	\label{eq:regularized_family}
\end{equation}
where $\complexity(f)$ is a measure of complexity of~$f$. Consequently, $F_a
\subseteq F_b$ whenever~$a, b \in \reals{+}$ such that~$a < b$. ERM~and~SRM seek
to find the functions given by
\begin{equation}
	f_{\text{ERM}} \coloneqq \argmin_{f \in F} \hat{R}(f)
	\quad\text{and}\quad
	f_{\text{SRM}} \coloneqq \argmin_{\substack{c \in \reals{+} \\ f \in F_c}}
		\hat{R}(f),
	\label{eq:erm_srm_solutions}
\end{equation}
respectively. The SRM procedure finds a function in~$F$ that
minimizes~(\ref{eq:generalization_bound})~(see Figure~\ref{fig:srm_graph}). For
convenience, we can transform second equation in~(\ref{eq:erm_srm_solutions})
into an unconstrained minimization problem by using the penalty method. This
gives
\begin{equation}
	f_{\text{SRM}} = \argmin_{\substack{\lambda \in \reals{+} \\ f \in F}}
		\left(\hat{R}(f) + \lambda\complexity(f)\right).
	\label{eq:srm_penalty}
\end{equation}
The Lagrange multiplier~$\lambda$ called the \emph{regularization parameter},
and penalizes more complex functions. We can now conduct our search
over~$\lambda$ instead of over~$c$. If~$H$ is a vector space, then we can define
$\complexity(f) \coloneqq \|f\|$ for some norm~${\|\cdot\|}$. Note that SRM
involves finding the solution to several ERM subproblems, and is generally
intractable. In practice, we find an \emph{approximate} SRM solution by using
cross-validation to choose~$\lambda^\star$ from a finite sequence~$\{ \lambda_k
\}_{k = 1}^r$.

As an example, suppose that we are are performing least-squares regression,
where~$X = \reals{n}$ and~$Y \in \reals{1}$. Let~$S$ be a training sample
consisting of~$s$ instances. We define
\begin{equation}
	F \coloneqq \{ x \mapsto \trans{w}x + b : w \in \reals{n}, b \in
		\reals{1} \},
	\label{eq:affine_family}
\end{equation}
and assume let~$E$ be the sum-of-squares error given
by~(\ref{eq:sum_of_squares}). According to the theory developed so far, choosing
$f \coloneqq \argmin_{f \in F} E(f)$ is highly inadvisable. Let $\complexity(f)
\coloneqq \|w\|_2$ for any $f \in F$. A better idea would be to try sequence of
values for~$\lambda$ in~(\ref{eq:srm_penalty}), such as~$\{ \lambda_k \}_{k =
-5}^5$, where~$\lambda_k \coloneqq 2^k$. In this way, the results from
statistical learning theory shape our perspective when solving practical
problems.

\section{Characterizing Optimization Algorithms}
\label{sec:characterizing_opt}

\subsection{Goals of Optimization}

We turn our focus to applying the results
from~Section~\ref{src:concepts_from_slt} to characterize the quality of
optimization algorithms for machine learning. To more closely model reality, we
split~$\esterr$ into two terms, yielding a threefold decomposition
of~$\mathcal{E}$. Let~$f_{\text{ERM}}$ be given by~(\ref{eq:erm_srm_solutions}).
During training, we may only require that the optimization algorithm produce a
function~$\hat{f} \in F$, whose empirical error is within a fixed
tolerance~$\rho \in \reals{+}$ of that of~$f_{\text{ERM}}$:
\begin{equation}
	\hat{R}(\hat{f}) - \hat{R}(f_{\text{ERM}}) \leq \rho.
	\label{eq:tol_eq}
\end{equation}
The expectation of the LHS of~(\ref{eq:tol_eq}) taken over a sample~$S \sim D$
is called the \emph{optimization error}~\citep{bousquet2008tradeoffs}, and is
defined as
\[
	\opterr \coloneqq R(\hat{f}) - R(f_{\text{ERM}}).
\]
We can now express the excess error of~$\hat{f}$ as
\begin{align}
	\mathcal{E}
	&= (R(f_F^\star) - R(f^\star)) + (R(f_{\text{ERM}}) - R(f_F^\star)) +
		(R(\hat{f}) - R(f_{\text{ERM}})) \\
	&= \apperr + \esterr + \opterr.
	\label{eq:excess_error}
\end{align}

In practice, the quality of the solution to a machine learning problem is
determined by monetary cost (which determines the maximum sample
size~$s_{\text{max}}$) and human patience (which determines the limit on
computation time~$t_{\text{max}}$). The definition for PAC-learning is one way
of characterizing when one might be satisfied with the performance of a learning
algorithm with respect to these constraints. Let~$t(F, \rho, s)$ represent the
time required by the optimization algorithm to find a function~$\hat{f} \in F$
satisfying~$\opterr \leq \rho$, given a sample of size~$s$. Minimizing the
excess error in~(\ref{eq:excess_error}) subject to the constraints
on~$s_{\text{max}}$ and~$t_{\text{max}}$ yields the following meta-optimization
problem~\citep{bousquet2008tradeoffs}:
\begin{equation}
\begin{aligned}
	& \underset{F, \rho, s}{\text{minimize}} & &
		\mathcal{E} = \apperr + \esterr + \opterr \\
	& \text{subject to} & & \hspace{3.2em} s \leq s_{\text{max}} \\
	& & & t(F, \rho, s) \leq t_{\text{max}}.
\end{aligned}
\label{eq:metaopt_program}
\end{equation}
Table~\ref{tab:metaopt_variables} shows how the errors and computing time are
affected when $\capacity(F)$, $\rho$, and~$s$ are increased.

\begin{table}
\centering
\begin{tabular}{lccc}
\toprule
& $\capacity(F)$ & $s$ & $\rho$ \\
\midrule
$\apperr$ & $\downarrow$ & \text{---} & \text{---} \\
$\esterr$ & $\updownarrow$ & $\updownarrow$ & \text{---} \\
$\opterr$ & $\uparrow$ & $\uparrow$ & $\downarrow$ \\
$t$ & $\uparrow$ & $\uparrow$ & $\downarrow$ \\
\bottomrule
\end{tabular}
\caption{The effects of increasing $\capacity(F)$, $s$, and~$\rho$ on the three
errors and computing time. The symbol $\updownarrow$ indicates that either an
increase or a decrease is possible.\label{tab:metaopt_variables}}
\end{table}

As stated in~\citet{bousquet2008tradeoffs}, ``the solution of the optimization
program~(\ref{eq:metaopt_program}) depends critically on which budget constraint
is active: [the] constraint~$s < s_{\text{max}}$ on the number of examples, or
[the] constraint~$t < t_{\text{max}}$ on the training time''. A
\emph{small-scale} problem is one that is constrained by the sample size. In
this case, the computation time is not limited, so we can make~$\opterr$
negligible by choosing~$\rho$ sufficiently small. Using the fact that~$\esterr$
is bounded above by the second term of~(\ref{eq:generalization_bound}), we have
\begin{equation}
	\mathcal{E} = \apperr + \esterr \leq \apperr + c
		\sqrt{\frac{\capacity(F)}{s}}.
	\label{eq:small_scale_bound}
\end{equation}
Hence minimizing~$\mathcal{E}$ boils down to the discussion in
Section~\ref{sec:erm_vs_srm}. A~\emph{large-scale} problem is one that is
constrained by the maximum computing time. In this case, performing approximate
optimization by choosing~$\rho > 0$ improves generalization, because we can
process more training instances in the allotted time.
Using~(\ref{eq:generalization_bound}), it is easy to show that
\begin{equation}
	\mathcal{E}
	= \apperr + \esterr + \opterr
	\leq \apperr + \rho + c\sqrt{\frac{\capacity(F)}{s}}.
	\label{eq:large_scale_bound}
\end{equation}

Unfortunately, the bounds~(\ref{eq:small_scale_bound})
and~(\ref{eq:large_scale_bound}) are too pessimistic, and do not accurately
reflect reality. In order to proceed with our analysis, we make some simplifying
assumptions. First, we assume that $F$~is the family of affine functions given
by~(\ref{eq:affine_family}). In this special case, the generalization
bound~(\ref{eq:generalization_bound}) simplifies to
\[
	R(f) \leq \hat{R}(f) + c \sqrt{\frac{n}{s}}.
\]
A much sharper bound on the excess error can now be
applied~\citep{bousquet2008tradeoffs}. It is given by
\begin{equation}
	\mathcal{E}
	= \apperr + \esterr + \opterr
	\leq c \left( \apperr + \left(\frac{n}{s}\log\frac{s}{n}\right)^\alpha +
		\rho \right),
	\label{eq:fast_rate_bound}
\end{equation}
where~$\alpha \in [1/2, 1]$ is a constant that depends on a variance bound on
the loss function. Since the three components of the excess error should
decrease at approximately the same rate~\citep{bousquet2008tradeoffs}, we make
the additional assumption that
\begin{equation}
	\mathcal{E} \approx \apperr \approx \esterr \approx \opterr \approx
		\left(\frac{n}{s}\log\frac{s}{n}\right)^\alpha \approx \rho.
	\label{eq:error_equiv}
\end{equation}

\subsection{Analysis of Optimization Algorithms}

We now investigate the properties of four specific optimization algorithms.
Assume that our empirical loss function~$E$ is convex and twice differentiable
(e.g. the sum-of-squares loss function). For convenience, we let~$\theta \in
\reals{2n}$, where the first~$n$ components of~$\theta$ refer to a weight
vector~$w \in \reals{n}$ and the last~$n$ components of~$\theta$ refer to a bias
vector~$b \in \reals{n}$. Thus~$\theta = (w, b)$, for some~$w, b \in \reals{n}$.
We define~$E(\theta) \coloneqq E(w, b)$. Since~$E$ is convex, it has a unique
minimum in terms of~$\theta$. Let~$\theta_{\text{ERM}} \coloneqq
(w_{\text{ERM}}, b_{\text{ERM}})$ be the parameter vector corresponding to
$f_{\text{ERM}}$. For a given input~$x_k \in \reals{n}$, we define
$\hat{y}_k^{\text{ERM}} \coloneqq w_{\text{ERM}} x_k + b_{\text{ERM}}$.

The Hessian matrix~$H_{\text{ERM}}$ and gradient covariance
matrix~$G_{\text{ERM}}$ evaluated at~$\theta_{\text{ERM}}$ play an important
role in our analyses~\citep{bousquet2008tradeoffs}. They are defined as
\[
	H_{\text{ERM}} \coloneqq (\nabla_\theta^2 E)(\theta_{\text{ERM}})
	= \frac{1}{s} \sum_{i = 1}^s (\nabla_\theta^2 e)(\hat{y}_k^{\text{ERM}}, y_k)
\]
and
\begin{align*}
	G_{\text{ERM}}
	&\coloneqq \left((\nabla_\theta E)(\theta_{\text{ERM}})\right)
		\trans{\left((\nabla_\theta E)(\theta_{\text{ERM}})\right)} \\
	&= \frac{1}{s} \sum_{i = 1}^s
		\left((\nabla_\theta e)(\hat{y}_k^{\text{ERM}}, y_k)\right)
		\trans{\left((\nabla_\theta e)(\hat{y}_k^{\text{ERM}}, y_k)\right)},
\end{align*}
where~$e$ is the per-instance loss function associated with~$E$. We assume that
there exist constants~$\lambda_{\text{max}} \geq \lambda_{\text{min}} > 0$
and~$\nu > 0$ such that for any $\delta \in (0, 1]$, there exists a sample
size~$s \in \naturals{1}$ for which
\[
	\tr\left( G_{\text{ERM}} (H_{\text{ERM}})^{-1} \right) \leq \nu
	\quad\text{and}\quad
	\spectrum(H_{\text{ERM}}) \subset [\lambda_{\text{min}}, \lambda_{\text{max}}],
\]
with probability at least~$1 - \delta$. The condition number~$\kappa \coloneqq
\lambda_{\text{max}}/{\lambda_{\text{min}}}$ determines the difficulty of the
optimization problem.

The optimization algorithms used in machine learning either operate on
\emph{full batches} or \emph{mini-batches}. An full batch algorithm will only
apply an update to~$\theta$ after processing the \emph{entire} training
sample~$S$. The derivative information computed by batch algorithms is averaged
over the batch of instances. A mini-batch algorithm applies an update to
$\theta$ every~$r$ iterations, where~$r | s$. A \emph{stochastic} or
\emph{online} algorithm uses a mini-batch size of one. The first two algorithms
that we present are full batch algorithms, while the last two algorithms are
stochastic.

Mini-batch algorithms are often several times faster than full batch algorithms.
This is because mini-batch algorithms are better able to exploit redundancy in
the training sample~$S$. \citet{lecun-98b} give an explanation for why this
happens. Suppose that~$|S| = \num{1000}$, and consists of the same~100 instances
replicated ten times in the same order. The average of the gradient over~$S$ is
equal to the average of the gradient over the first 100~instances. A full batch
algorithm will compute the same gradient ten times before applying a single
update. In this time, an algorithm with a mini-batch size of~100 would apply ten
updates. While duplicate instances are rare in practice, there are often
clusters of instances that are similar in some sense.

Following the precedent of Stephen Wright, we refer to the algorithms below as
\emph{gradient update} algorithms rather than as \emph{gradient descent}
algorithms, because the search directions used by the update rules are not
necessary descent directions.
\begin{itemize} 
\item The gradient update~(GU) algorithm uses the update rule
\[
	\theta_{k + 1}
	\coloneqq \theta_k - \eta (\nabla_\theta E)(\theta_k)
	= \theta_k - \frac{\eta}{s} \sum_{i = 1}^s
		(\nabla_\theta e)(\hat{y}_k^{\text{ERM}}, y_k)
\]
When~$\eta = 1/\lambda_{\text{max}}$, this algorithm requires~$O(\kappa \log(1 /
\rho))$ updates to reach accuracy~$\rho$.

\item The second-order gradient update~(2GU) algorithm uses the update rule
\[
	\theta_{k + 1}
	\coloneqq \theta_k - (H_{\text{ERM}})^{-1} (\nabla_\theta E)(\theta_k)
	= \theta_k - \frac{(H_{\text{ERM}})^{-1}}{s} \sum_{i = 1}^s
		(\nabla_\theta e)(\hat{y}_k^{\text{ERM}}, y_k)
\]
Note that the Hessian is \emph{not} evaluated at each~$\theta_k$: we assume that
we know~$H_{\text{ERM}}$ in advance, and use the same matrix at each iteration.
In general, this algorithm requires~$O(\log\log(1 / \rho))$ iterations to reach
accuracy~$\rho$.

\item Given an instance~$(x_k, y_k) \in S$, the stochastic gradient update~(SGU)
algorithm uses the update rule
\[
	\theta_{k + 1}
	\coloneqq \theta_k - \frac{\eta}{k}
		(\nabla_\theta e)(\hat{y}_k^{\text{ERM}}, y_k).
\]
If~$\eta = 1/\lambda_{\text{min}}$, this algorithm requires~$\nu \kappa^2 / \rho
+ o(1 / \rho)$ iterations, on average, to reach accuracy~$\rho$.

\item Given an instance~$(x_k, y_k) \in S$, the second-order stochastic gradient
update algorithm~(2SGU) uses the update rule
\[
	\theta_{k + 1}
	\coloneqq \theta_k - \frac{(H_{\text{ERM}})^{-1}}{k}
		(\nabla_\theta e)(\hat{y}_k^{\text{ERM}}, y_k).
\]
This algorithm requires~$\eta / \rho + o(1 / \rho)$ iterations, on average, to
reach accuracy~$\rho$. Note that, unlike~2GU, the use of second-order
information does not weaken the dependence on~$\rho$.
\end{itemize}

\begin{table}
\centering\footnotesize
\begin{tabular}{
	>{\raggedright\arraybackslash}m{1.5cm}
	>{\centering\arraybackslash}m{2cm}
	>{\centering\arraybackslash}m{2cm}
	>{\centering\arraybackslash}m{3.25cm}
	>{\centering\arraybackslash}m{3.25cm}
}
\toprule
Algorithm &
Cost of one iteration &
Iterations to reach~$\rho$ &
Time to reach accuracy~$\rho$ &
Time to reach $\mathcal{E} \leq c(\apperr + \epsilon)$ \\
\midrule
GU &
$O(ns)$ &
$O\left( \kappa \log\frac{1}{\rho} \right)$ &
$O\left( ns\kappa \log\frac{1}{\rho} \right)$ &
$O\left( \frac{n^2\kappa}{\epsilon^{1 / \alpha}} \log^2 \frac{1}{\epsilon} \right)$ \\
2GU &
$O(n^2 + ns)$ &
$O\left( \log\log\frac{1}{\rho} \right)$ &
$O\left( (n^2 + ns) \log\log\frac{1}{\rho} \right)$ &
$O\left( \frac{n^2}{\epsilon^{1 / \alpha}} \log \frac{1}{\epsilon} \log\log
	\frac{1}{\epsilon} \right)$ \\
SGU &
$O(n)$ &
$\frac{\nu \kappa^2}{\rho} + o\left(\frac{1}{\rho}\right)$ &
$O\left( \frac{n \nu \kappa^2}{\rho} \right)$ &
$O\left( \frac{n \nu \kappa^2}{\epsilon} \right)$ \\
2SGU &
$O(n^2)$ &
$\frac{\nu}{\rho} + o\left( \frac{1}{\rho} \right)$ &
$O\left( \frac{n \nu}{\rho} \right)$ &
$O\left( \frac{n \nu}{\epsilon} \right)$ \\
\bottomrule
\end{tabular}
\caption{Asymptotic rates for the four algorithms discussed (adapted
from~\citet{bousquet2008tradeoffs}).\label{tab:asymptotic_rates}}
\end{table}

The asymptotic rates for the four algorithms that were presented are given in
Table~\ref{tab:asymptotic_rates}. The rates in the first column are based on the
sizes of the vectors and matrices involved in the corresponding update rules.
The rates in the second column are taken directly from our discussion. To obtain
the rates in the third column, we multiply the corresponding rates in the first
and second columns. One of our assumptions in~(\ref{eq:error_equiv}) is that
\begin{equation}
	\rho \approx \left(\frac{n}{s}\log\frac{s}{n}\right)^\alpha.
	\label{eq:rho_assumption}
\end{equation}
Substituting this value of~$\rho$ into the bound~(\ref{eq:fast_rate_bound}) and
comparing the result to~$c(\apperr + \epsilon)$ shows that
\begin{equation}
	\rho \approx \epsilon.
	\label{eq:rho_approx_eps}
\end{equation}
Using~(\ref{eq:rho_assumption}) and~(\ref{eq:rho_approx_eps}), it can then be
shown that
\begin{equation}
	s \approx \frac{n}{\epsilon^{1 / \alpha}} \log\frac{1}{\epsilon}.
	\label{eq:s_assumption}
\end{equation}
Substituting~(\ref{eq:rho_approx_eps}) and~(\ref{eq:s_assumption}) into the rates
in the third column gives the rates in the fourth column.

Table~\ref{tab:asymptotic_rates} contains a wealth of information regarding the
properties of the four optimization algorithms. This information leads to the
following conclusions for large-scale problems~\citep{bousquet2008tradeoffs}:
\begin{itemize}
\item Stochastic algorithms are more appropriate when there is less need to
optimize accurately. This is the case in large-scale problems, or when the
estimation rate~$\alpha$ is small. Note that the performance of the stochastic
algorithms does not depend on the estimation rate~$\alpha$.

\item Hastily incorporating second-order information into gradient update
algorithms may only bring small improvements in~$\mathcal{E}$. When
minimizing~$\mathcal{E}$, all four algorithms are dominated by the polynomial
factor in~$1/\epsilon$. The second-order methods shown here only improve the
logarithmic factors in~$1/\epsilon$.

\item Stochastic algorithms have the best generalization performance, despite
being the worst optimization algorithms when viewed in the traditional sense.
When minimizing~$\mathcal{E}$, the batch algorithms have a dependence
on~$\alpha$ as well as the logarithmic factor in~$1/\epsilon$.
\end{itemize}
For small-scale problems, $\opterr$~is negligible, so these conclusions do not
necessarily apply.

One caveat is that stochastic algorithms are more vulnerable than batch
algorithms to ill-conditioning. The results for both stochastic algorithms
depend on the constant~$\nu$. The factor of~$\kappa$ in the results for GU
becomes~$\kappa^2$ in the results for SGU. As~$H_{\text{ERM}}$ becomes
increasingly ill-conditioned, both $\kappa$~and~$\nu$ are liable to increase
rapidly. One of the goals of decorrelating the
inputs~(Section~\ref{sec:pp_and_init}) is to prevent~$H_{\text{ERM}}$ from
having large eigenvalues~\citep{lecun-98b}. In light of the results from
Table~\ref{tab:asymptotic_rates}, the importance of decorrelating the inputs
becomes even more apparent.

\section{Optimization Algorithms for Neural Networks}

\subsection{Challenges of Large-Scale Optimization}

Training convolutional networks is prototypical of the large-scale problems
discussed in the previous section. Our guidelines suggest that for such
problems, optimization algorithms based on steepest descent should use
mini-batches, and carefully evaluate the tradeoffs of incorporating second-order
information. Almost all optimization algorithms used to train convolutional
networks, or deep neural networks of other kinds, abide by these constraints.

First-order algorithms must effectively deal with the vanishing gradient problem
and the issue of ``pathological curvature''~\citep{martens2010deep}. In
Section~\ref{sec:pp_and_init}, we mentioned that activation values in deeper
layers of a network have a greater tendency to cluster around zero or very large
values. Since the gradients in one layer depend multiplicatively on those in the
next~(see Section~\ref{sec:computing_derivatives}), the vanishing gradient
problem usually affects the lowest levels most severely. We also saw that when
features of the input are correlated, the weight updates are no longer
decoupled. Both of these phenomena suggest that \emph{each} weight and bias in
the network should be given its own learning rate~\citep{lecun-98b}, something
that is usually not done for traditional optimization problems.

The asymptotic rates for the stochastic algorithms in
Table~\ref{tab:asymptotic_rates} suggest vulnerability to ill-conditioning of
the Hessian matrix, especially when second-order information is not used. The
problem is in fact much more serious than this. Suppose that each weight and
bias in the network is given its own learning rate. By viewing the full batch
gradient update rule as a discrete time dynamical system, it can be shown that
the optimal learning rate for the $i$th parameter $\theta_i$ scales the
corresponding eigenvalue of the Hessian matrix to unity~\citep{lecun-98b}. That
is, the optimal learning rate for $\theta_i$ is given by
\[
	\eta_i^\star \coloneqq \frac{r}{\lambda_i},
\]
where~$r$ is the size of the mini-batch. Intuitively, this means that we would
like to amplify~$\eta_i$ along directions of low curvature, and decay~$\eta_i$
along directions of high curvature.

Even for a small network, computing the spectrum of the Hessian matrix is
completely intractable. \citet{lecun-98b} give several techniques for estimating
the maximum eigenvalue of the Hessian matrix, but dividing all of the learning
rates by this quantity would unnecessarily retard progress. This means that
first-order algorithms must somehow deal with pathological curvature without
access to second-order information.

Unsurprisingly, the most successful ones do this by making innovative use of the
information that is available. These algorithms enjoy widespread use, and can
solve a variety of challenging tasks. However, there is a small but definite gap
in the performance between first- and second-order
algorithms~\citep{martens2010deep, ngiam2011optimization,
sutskever2013importance}. Second-order algorithms seem to make effective use of
the curvature information from Hessian matrix in order to explore regions of the
parameter space that are otherwise inaccessible. This gain in performance comes
at a cost: second-order algorithms are considerably more complex, and
practitioners are less willing to implement them.

\subsection{Momentum}

\begin{figure}[t]
\centering
\begin{subfigure}[t]{0.4\textwidth}
	\centering
	\includegraphics[width=\textwidth]{graphics/cm_diagram.pdf}
	\caption{The CM update rule.\label{fig:cm_update}}
\end{subfigure}
\begin{subfigure}[t]{0.4\textwidth}
	\centering
	\includegraphics[width=0.8\textwidth]{graphics/nag_diagram.pdf}
	\caption{The NAG update rule.\label{fig:nag_update}}
\end{subfigure}
\caption{Illustration of the two momentum-based update rules (adapted
from~\citet{sutskever2013importance}).\label{fig:momentum_updates}}
\end{figure}

Accelerated gradient methods have been the subject of much of the recent work in
convex optimization theory~\citep{sutskever2013importance}. Much of this work
has involved the analysis of classical momentum~(CM) and Nesterov accelerated
gradient~(NAG). For the purposes of our discussion, let~$E$ denote the loss
function, $\theta$~the parameters of the model, and~$k$ the current iteration.
The value of~$\theta$ at convergence is denoted by~$\theta^\star$. Lastly, we
define
\[
	g_k \coloneqq (\nabla_\theta E)(\theta_k)
	\quad\text{and}\quad
	H_k \coloneqq (\nabla^2_\theta E)(\theta_k).
\]

The~CM update rule is given by
\begin{align*}
	s_{k + 1} &\coloneqq \mu s_k - \eta g_k \\
	\theta_{k + 1} &\coloneqq \theta_k + s_{k + 1},
\end{align*}
where~$\mu \in [0, 1]$ is the \emph{momentum} parameter~(see
Figure~\ref{fig:cm_update}). The idea behind momentum is to ``smooth out'' the
highly oscillatory path of stochastic updates using an exponentially-decaying
average of past gradients. This dampens the wild oscillations that are
characteristic of the unstable nature of SGU~algorithms. It also amplifies the
gradient along directions of low curvature. In a deterministic setting, CM
requires $\sqrt{\kappa}$~times fewer iterations than steepest descent to reach a
solution of accuracy~$\rho$, where~$\kappa$ is defined as
in~Table~\ref{tab:asymptotic_rates}~\citep{sutskever2013importance}. Hence, CM
reduces the vulnerability of first-order algorithms to ill-conditioning of the
Hessian matrix.

NAG differs from CM in that it \emph{first} moves along the direction given by
the momentum term, and \emph{then} applies a correction based on the gradient at
that point~(see Figure~\ref{fig:nag_update}). This subtle change makes a large
difference in practice. The NAG update rule can be written
as~\citep{sutskever2013importance}:
\begin{align*}
	\hat{\theta}_{k + 1} &\coloneqq \theta_k + \mu s_k \\
	s_{k + 1}            &\coloneqq \mu s_k - \eta \hat{g}_{k + 1} \\
	\theta_{k + 1}       &\coloneqq \theta_k + s_{k + 1}.
\end{align*}
To see why NAG can be more effective than CM, suppose that~$E$ briefly decreases
along~$s_k$, but soon begins to rapidly increase. Subtracting $-\eta \hat{g}_{k
+ 1}$ from the momentum term helps us ``steer away'' from this poor update,
whereas subtracting $-\eta g_k$ does not~(see
Figure~\ref{fig:momentum_updates}). The correction applied by CM will only occur
at the next iteration, by which time we will have already moved along this bad
update. In a deterministic setting for a smooth, convex function, NAG achieves a
global convergence rate of~$O(1 / k^2)$, compared to convergence rate of~$O(1 /
k)$ for steepest descent~\citep{sutskever2013importance}.

Figure~\ref{fig:momentum_updates} suggests that there is a relationship
between~CM and~NAG. Suppose that~$s_k$ is an eigenvector of a quadratic function
with positive-definite Hessian matrix~$H$. Then the NAG~update is equivalent to
a~CM update with momentum parameter
\[
	\mu_{\text{CM}} \coloneqq \mu_{\text{NAG}} (1 - \eta \lambda),
\]
where~$\lambda$ is the eigenvalue of~$H$ corresponding to~$s_k$ and~$\eta \in
[0, 1 / \lambda]$~\citep{sutskever2013importance}. If~$\eta \approx 0$,
then~$\eta \lambda \approx 0$ for all~$\lambda \in \spectrum(H)$, and the
NAG~update is approximately the same as the CM~update. On the other hand,
if~$\eta \approx 1 / \lambda$, then~$\mu \approx 0$, so NAG uses a smaller
effective momentum than CM. This shows that NAG is able to automatically
throttle the momentum parameter along directions of high curvature, adding a
measure of stability that is absent in~CM.

When training deep neural networks, the \emph{local} convergence properties of
optimization algorithms are largely irrelevant. Since the parameter space for
deep neural networks is so vast and complex, the ``transitory
period''~\citep{sutskever2013importance} before local convergence sets in lasts
for almost the entire duration of the optimization process. It is in this
setting that the advantage of using momentum becomes clear.

Suppose that we are optimizing a smooth, convex function~$f : \reals{1} \to
\reals{1}$ using stochastic first-order algorithms. Without momentum (i.e. using
vanilla SGU), we have a convergence rate of~$O(\ell / k + \sigma / \sqrt{k})$,
where~$\ell$ is the Lipschitz constant and $\sigma$~is the variance in the
gradient estimate. With momentum, the convergence rate is~$O(\ell / k^2 + \sigma
/ \sqrt{k})$~\citep{sutskever2013importance}. Since $f$~is convex, the first
term is dominant during the transitory period, and momentum gives us a clear
advantage. As $\theta_k \to \theta^\star$, the second term takes effect, and
both algorithms have approximately the same performance.

\subsection{First-Order Optimization Algorithms}

\begin{figure}[t]
\centering
\resizebox{0.5\textwidth}{!}{%
	\includegraphics{graphics/first_order_hierarchy.pdf}
}
\caption{Hierarchy of the first-order optimization techniques and algorithms.
An arrow from one block to another means that the algorithm represented by the
first block is based on the one represented by the second. Here, CM~stands for
``classical momentum'', NAG~for ``Nesterov accelerated momentum'', and DH~for
``diagonal Hessian''.\label{fig:first_order_hierarchy}}
\end{figure}

All of the successful first-order optimization algorithms for neural networks
make use momentum in some form~(see Figure~\ref{fig:first_order_hierarchy}). The
first algorithm that we present is abbreviated DH~(for ``diagonal Hessian''),
and is based on a diagonal approximation to the Levenberg-Marquardt
algorithm~\citep{lecun-98b}. In the spirit of~CM, DH~updates an exponentially
decaying average~$\bar{h}_k \in \reals{n}$ of the diagonal of the Hessian
matrix. We define the division and absolute value operations for vectors so that
they are carried out elementwise. The update rule for~DH is given by
\begin{align*}
	\bar{h}_{k + 1} &\coloneqq (1 - \tau) \bar{h}_k + \tau |\text{diag}(H_k)| + \epsilon \\
	\theta_{k + 1}  &\coloneqq \theta_k - \frac{\eta}{\bar{h}_{k + 1}} \;g_k,
\end{align*}
where the \emph{decay} parameter~$\tau \in [0, 1]$ controls the ``momentum'' in
the running average, and~$\epsilon \in \reals{n}$ is a nonnegative constant
vector chosen to prevent the effective learning rate from growing too large.
Using~$\diag(H_k)$ to scale the individual learning rates can outperform
carefully annealed versions of~SGU that use global learning
rates~\citep{lecun-98b}.

The ``variance-based stochastic gradient descent''~(vSGD) also
uses~$\diag(H_k)$, but incorporates several other features for added robustness.
It is derived by optimizing the expected value of the per-sample loss based on a
local quadratic model with diagonal Hessian matrix~\citep{schaul-icml-13}.
Let~$e \in \reals{n}$ be the constant vector of ones, and let~$\odot$ denote the
elementwise product operation. We define exponentiation for vectors so that the
operation is carried out elementwise. The~vSGD algorithm maintains exponentially
decaying averages of the gradient, squared gradient, and Hessian diagonal
vectors~$\bar{g}, \bar{v}, \bar{h} \in \reals{n}$. These vectors are updated
according to the rules
\begin{align*}
	\bar{g}_{k + 1} &\coloneqq (e - \tau^{-1}) \odot \bar{g}_k + \tau^{-1} \odot g_k \\
	\bar{v}_{k + 1} &\coloneqq (e - \tau^{-1}) \odot \bar{v}_k + \tau^{-1} \odot (g_k)^2 \\
	\bar{h}_{k + 1} &\coloneqq (e - \tau^{-1}) \odot \bar{h}_k + \tau^{-1} \odot \diag(H_k)
		+ \epsilon,
\end{align*}
where~$\epsilon \in \reals{n}$. The learning rate and decay parameters~$\eta,
\tau \in \reals{n}$ are updated according to the rules
\begin{align*}
	\eta_{k + 1} &\coloneqq \frac{(\bar{g}_{k + 1})^2}
		{\bar{h}_{k + 1} \odot \bar{v}_{k + 1}}
	= (\bar{h}_{k + 1})^{-1} \odot
		\frac{(\bar{g}_{k + 1})^2}{\bar{v}_{k + 1}} \\
	\tau_{k + 1} &\coloneqq \left( e - \frac{(\bar{g}_{k + 1})^2}{\bar{v}_{k + 1}} \right)
		\odot \tau_k + 1.
\end{align*}
The factor~$(\bar{g}_{k + 1})^2 / \bar{v}_{k + 1}$ in the update to~$\eta$
scales the effective learning rate to compensate for the vanishing gradient
problem, while the factor~$(\bar{h}_{k + 1})^{-1}$ attempts to correct for
pathological curvature. The update to~$\tau$ is chosen such that the momentum
increases when the step size decreases, and decreases when the step size
increases. Lastly, the update to~$\theta$ is given by
\[
	\theta_{k + 1} \coloneqq \theta_k - \eta_{k + 1} \odot g_k.
\]

While the~vSGD update rule is complex, it is designed to work effectively with a
minimal amount of human effort. Previously proposed methods had on the order of
ten or a hundred hyperparameters, and were often sensitive to the values chosen
for them. Additionally, first-order methods often require careful annealing of
the learning rate to yield acceptable results, or sometimes to converge at
all~\citep{krizhevsky2012imagenet}. The~vSGD algorithm only has a single
parameter that needs to be set~(hence the name of the paper ``No More Pesky
Learning Rates''), and the learning rate automatically converges to zero as the
value of the loss function becomes optimal. If the objective function is
nonstationary and the data changes, vSGD will ramp up the learning rate in
response~\citep{schaul-icml-13}. This makes vSGD suitable for online learning
problems.

The~vSGD algorithm works best when initialized with a \emph{slow start}
heuristic~\citep{schaul-icml-13}. By this, we mean that the parameter updates
should be kept small artificially until the exponentially decaying averages
become sufficiently accurate. We first seed the algorithm by computing the
relevant averages over a ``handful'' of instances (the paper suggests~$10^{-3}
\cdot s$, where~$s$ is the size of the training sample). In order to keep the
initial parameter updates small, the resulting value of~$\bar{v}_1$ is
``overestimated'' by a factor~$C$, which the paper recommends setting to~$n /
10$, where~$n$ is the dimension of the input space. This is the only parameter
that needs to be set. Using this heuristic, the~vSGD algorithm surpasses the
performance of finely-tuned implementations of SGU and previously proposed
adaptive learning algorithms~\citep{schaul-icml-13}.

The ``root mean square propagation''~(RMSPROP) algorithm is based on a
\emph{full-batch} gradient update algorithm called ``resilient
backpropagation''~(RPROP)~\citep{hinton-nnml-2014}. The RPROP algorithm
maintains a separate learning rate for each parameter, and uses the update rule
\begin{align*}
	\theta_{k + 1} &\coloneqq \theta_k - \eta_{k + 1} \odot \sign(g_k),
\end{align*}
where~$\eta_1 = \eta_2 = e$, and
\[
	(\eta_{k + 1})_i \coloneqq
	\begin{cases} 
	\max(\eta_{\text{max}}, \gamma_e (\eta_k)_i) &
		\text{if $\sign((g_{k - 1})_i) = \sign((g_k)_i)$} \\
	\min(\eta_{\text{min}}, \gamma_c (\eta_k)_i) &
		\text{otherwise.}
	\end{cases}
\]
The constants $\gamma_c, \gamma_e \in \reals{+}$ are the \emph{contraction} and
\emph{expansion} factors, respectively. They must satisfy~$\gamma_c < 1$
(e.g.~$0.5$) and~$\gamma_e > 1$~(e.g. 1.2). The constants~$\eta_{\text{min}},
\eta_{\text{max}} \in \reals{+}$ limit the growth of the learning rates so that
they do not become ``too large'' or ``too small''; typical settings
are~$\eta_{\text{min}} = 10^{-6}$ and~$\eta_{\text{max}} = 50$.

The RPROP algorithm's strategy of only using the signs of the gradient vector is
also its limitation: RPROP cannot be used with mini-batches. The signs of the
gradient vector are only meaningful if the gradient over the current batch is a
good approximation of the true gradient of the loss function~$E$, which is
evaluated over the entire training sample. To see how things can go wrong,
suppose that RPROP were used as a stochastic algorithm. If the gradient of a
parameter~$\theta_i$ is~$+0.1$ for nine instances and~$-0.9$ for one instance,
then we would like the update to~$\theta_i$ to be close to zero. But RPROP will
increase~$\theta_i$ nine times, and only decrease~$\theta_i$
once~\citep{hinton-nnml-2014}.

The RMSPROP algorithm addresses this deficiency by scaling each component of the
gradient by the \emph{root mean square} of its exponentially decaying average.
This leads to the update rule
\begin{align*}
	\bar{v}_{k + 1} &\coloneqq (1 - \tau) \bar{v}_k + \tau (g_k)^2 + \epsilon \\
	\theta_{k + 1}  &\coloneqq \theta_k - \frac{\eta}{\sqrt{\bar{v}_{k + 1}}} \; g_k,
\end{align*}
where~$\epsilon \in \reals{n}$ and~$\tau, \eta \in \reals{+}$ such that~$\tau
\in [0, 1]$. The square root operation is performed elementwise. Incorporating
NAG into RMSPROP can sometimes be beneficial~\citep{hinton-nnml-2014,
climin-rmsprop}. This leads to the update rule
\begin{align*}
	\bar{v}_{k + 1}      &\coloneqq (1 - \tau) \bar{v}_k + \tau (g_k)^2 + \epsilon \\
	\hat{\theta}_{k + 1} &\coloneqq \theta_k + \mu s_k \\
	s_{k + 1}            &\coloneqq \frac{\eta}{\sqrt{\bar{v}_k}} \; \hat{g}_{k + 1} \\
	\theta_{k + 1}       &\coloneqq \theta_k + s_{k + 1}.
\end{align*}
For reasons that are currently unknown, the use of momentum with RMSPROP does
not help as much as it usually does for other first-order
algorithms~\citep{hinton-nnml-2014}.

The ADADELTA algorithm~\citep{zeiler2012adadelta} is a modification of the
ADAGRAD algorithm based on RMSPROP, although Zeiler did not seem to know of
RMSPROP by that name at the time of publication. The ADAGRAD algorithm uses the
update rule
\begin{align*}
	\bar{v}_{k + 1} &\coloneqq \bar{v}_k + (g_k)^2 + \epsilon \\
	\theta_{k + 1}  &\coloneqq \theta_k - \frac{\eta}{\sqrt{\bar{v}_{k + 1}}} \; g_k,
\end{align*}
where~$\epsilon \in \reals{n}$ and~$\eta \in \reals{+}$. This update rule is the
same as that of RMSPROP, except that the average of the gradient vector is not
exponentially decaying. As a result, the effective learning rate can only
decrease over time. If~$\eta$ is chosen too small, then progress will be
painfully slow. ADADELTA makes the running average of~$(g_k)^2$ exponentially
decreasing, and also multiplies the step by the root mean square of the average
of the past updates~(see \citet{zeiler2012adadelta} for the motivation behind
why this is done). These modifications lead to the update rule
\begin{align*}
	\bar{v}_{k + 1} &\coloneqq (1 - \tau) \bar{v}_k + \tau (g_k)^2 + \epsilon \\
	s_{k + 1}       &\coloneqq \sqrt{\frac{\bar{s}_k}{\bar{v}_{k + 1}}} \; g_k \\
	\bar{s}_{k + 1} &\coloneqq (1 - \tau) \bar{s}_k + \tau (s_{k + 1})^2 + \epsilon \\
	\theta_{k + 1}  &\coloneqq \theta_k - s_{k + 1}.
\end{align*}

The ADADELTA algorithm possesses many of the same properties as~vSGD. The
numerator~$s_k$ of the step has an effect similar to that of momentum, while the
denominator~$\bar{v}_{k + 1}$ scales the learning rates to compensate for the
vanishing gradient problem. Since the value~$s_k$ lags behind by an iteration,
ADADELTA is robust to sudden changes in the gradient, much in the same way
as~NAG. As $\theta_k \to \theta^\star$, $\bar{s}_k, \bar{v}_{k + 1} \to
\epsilon$, and~$\sqrt{\bar{s}_k / \bar{v}_{k + 1}} \to e$. This property
provides an intrinsic annealing schedule. Unfortunately, the updates that are
generated as~$\theta_k \to \theta^\star$ are still too aggressive, and local
convergence is inferior to that of a carefully tuned version of~SGU with
momentum.

\subsection{Second-Order Optimization Algorithms}
\label{sec:second_order_alg}

The two most successful optimization algorithms for deep neural networks that
can justifiably be called ``second-order methods'' are~L-BFGS and an adaptation
of the linear~CG method called ``Hessian-free
optimization''~(HF)~\citep{martens2010deep}. Both of these algorithms are
second-order methods from traditional optimization that have been adapted for
use with mini-batches. The~L-BFGS algorithm maintains a history of past values
of~$\theta_k$ and~$g_k$ in order to implicitly compute approximate
Hessian-vector products. Since the Hessian matrix is only crudely approximated
by past updates, accuracy can suffer compared to that of~HF, which computes
\emph{exact} Hessian-vector products~\citep{ngiam2011optimization}.

HF is the \emph{only} known competitive optimization algorithm for deep neural
networks that computes the Hessian-vector products exactly. At each outer
iteration, HF computes the gradient over a relatively large mini-batch. It then
iterates the linear~CG method to minimize the local quadratic model determined
by the gradient and Hessian. At each inner iteration of~CG, a modified version
of backpropagation~(see Section~\ref{sec:computing_derivatives}) is used to
compute the Hessian-vector products exactly. These inner iterations are
terminated when the relative per-iteration progress becomes sufficiently small.

In the process of adapting linear~CG to work for deep neural networks, Martens
developed custom strategies for damping, choosing the mini-batch size,
termination, ``hot-starting'' the inner iterations, preconditioning,
initialization, and more~\citep{martens2010deep}. \citep{martens-hf-guide}~give
a comprehensive tutorial on using HF in practice. While implementing HF is
involved, its performance in several tasks is still
unmatched~\citep{martens2010deep, ngiam2011optimization,
sutskever2013importance}. Unfortunately, practitioners are reluctant to
implement it due to its underlying complexity, instead preferring to continue
experimentation with first-order methods in order to tighten the performance
gap~\citep{sutskever2013importance}.

\section{Experiments}

I benchmarked the first-order methods covered in this report on the~MNIST
database of handwritten digits~\citep{lecun-98}. This data set consists
of~$\num{60000}$, $28 \times 28$, 8-bit, black and white images of handwritten
digits. The training set consists of~$\num{50000}$ images, and the test set
consists of the remaining~$\num{10000}$ images. Classifying handwritten digits
is now considered to be an easy task, since relatively small convolutional
networks can achieve error rates considerably less than one percent without any
data augmentation techniques~(e.g. affine distortions). Nonetheless, the~MNIST
database is still a useful benchmark that is almost invariably used in
publications that propose new techniques for convolutional networks.

The convolutional network that is used for this benchmark is the same one
described by~\citet{lecun-98}, except some small changes are made for the sake
of simplicity. Instead of using the given sparse connection pattern between
layers~S2 and~C3, I connected each unit in layer~C3 to the corresponding units
in every feature map of layer~S2. Additionally, the center of each Euclidean
RBF~unit was randomly initialized from the values~$\pm 1$ instead of being
chosen to represent ASCII bitmaps of the corresponding digit. The images in the
training sample were centered and scaled as described
by~(\ref{eq:center_and_scale}), and the initial weights of each layer were
sampled from a uniform distribution with mean zero and standard deviation given
by~(\ref{eq:weight_stddev}).

I used four variants each of~SGU: one with a global, static learning rate, one
with a global learning rate and an annealing schedule, one with~CM, and
with~NAG. The global learning rate was chosen by performing a grid search
in~$(0, 2]$, using increments of~$\rho_1 \coloneqq 0.1$. For each trial learning
rate, the convolutional network was trained on a randomly chosen sample of~$200$
training instances to minimize the test error. I identified the learning
rate~$\eta_1^\star$ achieving the lowest mean test error computed from five
initializations, each using a newly-chosen training sample. The grid search was
refined twice: first on the interval~$[\eta_1^\star - \rho_1, \eta_1^\star +
\rho_1]$, with~$\rho_2 \coloneqq 0.01$, and next on the interval~$[\eta_2^\star
- \rho_2, \eta_2^\star + \rho_2]$ with~$\rho_3 \coloneqq 0.001$.

The annealing schedule I used is given by
\[
	\eta_k \coloneqq \frac{\eta_0}{1 + \beta k},
\]
as suggested by~\citep{bottou2012stochastic}. Many other annealing schedules are
used in practice; see~\citet{pylearn2-training} for more examples. The
parameters~$\eta_0^\star, \beta^\star$ were found using a twice-refined grid
search on~$(0, 2]^2$, using the same values of~$\rho_1, \rho_2, \rho_3$ given
previously. The annealing schedule for the momentum is the same one used
by~\citet{sutskever2013importance}, and is given by
\[
	\mu_k \coloneqq \min\left(
		1 - 2^{-1 - \log_2(\floor{sk / r} + 1)},
	\mu_{\text{max}}\right),
\]
where~$s$ is the size of the training sample~($\num{50000}$~in our case), $r$~is
the mini-batch size, and~$\mu_{\text{max}} \in \reals{+}$ limits the growth of
the momentum parameter. For both~CM and~NAG, I kept the learning rate~$\eta$
constant, as was done in~\citet{sutskever2013importance}.
Both~$\mu_{\text{max}}$ and~$\eta$ were selected using a twice-refined grid
search on~$(0, 2]$ using the same values for~$\rho_1, \rho_2, \rho_3$ given
previously. The parameters for the same four variants of~DH were chosen using
the same grid search procedures, and $\tau$~was selected using a twice-refined
grid search on~$(0, 1]$ with the same values of~$\rho_1, \rho_2, \rho_3$.

While vSGD, RMSPROP, and ADADELTA are claimed to require little human effort,
selecting the best values for the hyperparameters still requires a grid search.
To find the best values for~$\eta$ and~$\tau$ for RMSPROP, $\eta$, $\tau$,
and~$\mu$ for RMSPROP~+~NAG, and~$\tau$ for ADADELTA, I performed the usual
twice-refined grid search. The initial search intervals for all parameters
were~$(0, 1]$. To find the best value of~$C$ for~vSGD, I performed a grid search
in~$(0, \num{1000}]$, with~$\rho_1 \coloneqq 100$, $\rho_2 \coloneqq 10$,
and~$\rho_3 \coloneqq 1$. All hyperparameters were kept constant during
training. Due to time constraints, I set the mini-batch size~$r$ to~$200$ for
all of the algorithms tested. The best results obtained are given in
Table~\ref{tab:first_order_results}. Since computing time is not a determining
factor for the quality of the solution found, I allowed all algorithms to run
until convergence.

\begin{table}
\centering
\begin{tabular}{llllc}
\toprule
Algorithm & \multicolumn{3}{c}{Parameters} & Test Error~(\%) \\
\midrule
SGU~(global) & $\eta \coloneqq 0.006$ & & & 1.14 \\
SGU~(annealed) & $\eta_0 \coloneqq 0.013$ & $\beta \coloneqq 1.005$ & & 0.83 \\
SGU~+~CM & $\eta_0 \coloneqq 0.004$ & $\mu_{\text{max}} \coloneqq 0.982$ & & 0.78 \\
SGU~+~NAG & $\eta_0 \coloneqq 0.006$ & $\mu_{\text{max}} \coloneqq 0.995$ & & 0.76 \\
DH~(global) & $\eta \coloneqq 0.011$ & $\tau \coloneqq 0.634$ & & 1.11 \\
DH~(annealed) & $\eta_0 \coloneqq 0.014$ & $\tau \coloneqq 0.667$ & $\beta \coloneqq 1.012$ & 0.82 \\
DH~+~CM & $\eta \coloneqq 0.012$ & $\tau \coloneqq 0.661$ & $\mu_{\text{max}} \coloneqq 0.988$ & 0.78 \\
DH~+~NAG & $\eta \coloneqq 0.013$ & $\tau \coloneqq 0.665$ & $\mu_{\text{max}} \coloneqq 0.993$ & 0.75 \\
vSGD & $C \coloneqq 48$ & & & 0.74 \\
RMSPROP & $\eta \coloneqq 0.004$ & $\tau \coloneqq 0.792$ & & 0.76 \\
RMSPROP + NAG & $\eta \coloneqq 0.004$ & $\tau \coloneqq 0.810$ & $\mu \coloneqq 0.664$ & 0.76 \\
ADADELTA & $\eta \coloneqq 0.006$ & & & 0.78 \\
\bottomrule
\end{tabular}
\caption{Results for the first-order optimization algorithms on the MNIST
database.\label{tab:first_order_results}}
\end{table}

Several features of the results confirm our expectations. The adaptive
algorithms (vSGD, RMSPROP, and ADADELTA), which use separate learning rates for
each parameter, generally outperformed those that used global learning rates.
However, the differences in performance were not as great as I would have
expected. Each modification, such as the incorporation of momentum
or~$\diag(H)$, only yielded minuscule improvements in the test error at
convergence. Moreover, these improvements were only apparent after exhaustive
grid searches were used to find the best values for all of the hyperparameters. A
practitioner is unlikely to have the time or patience to do this for each new
problem. In this respect, the adaptive algorithms provide a clear advantage:
they are robust with respect to selection of the hyperparameters.

I was surprised to see that the heuristics used by the adaptive algorithms only
provided marginal improvements over the best results for algorithms that used
global learning rates. One explanation for this is that a finely-tuned annealing
schedule ensures that the effective learning rates gently approach zero
as~$\theta_k \to \theta^\star$. Both vSGD and ADAGRAD have intrinsic annealing
schedules, but the updates generated as~$\theta_k \to \theta^\star$ are probably
too aggressive. Further experimentation with custom annealing schedules is
likely to provide improvements over the results for the adaptive algorithms.
These results show that careful testing can provide valuable information about
the relative performance of algorithms that would otherwise be missed.

\section{Conclusion}

Research suffers from a lack of theoretical understanding of the training
dynamics of neural networks. Most of the techniques used by the recent adaptive
optimization algorithms for neural networks were already mentioned over a decade
earlier~\citep{lecun-98b}. But because the experiments carried out at that time
were not comprehensive enough, these techniques were quickly forgotten, and had
to be rediscovered later. The availability of more general software that would
allow practitioners to easily try out a variety of optimization algorithms for a
given problem may help avoid this problem.

Due to space and time constraints, several interesting topics were not addressed
by the experiments. The MNIST database was the only data set used for the
experiments. The extent to which the relative performance of optimization
algorithms depends on specific features of the data remains to be seen.  There
is also evidence that incorporating a line search into first-order optimization
algorithms, even for small mini-batches, may improve
generalization~\citep{ngiam2011optimization}. The interaction between line
search procedures and the other techniques used by these algorithms remains
unexplored. Lastly, none of the second-order algorithms mentioned in
Section~\ref{sec:second_order_alg} were implemented. For a different type of
convolutional network, \citet{sutskever2013importance} report a test error
of~$0.69\%$ on the MNIST database, using a modified version of~HF. It would be
interesting to incorporate the results for~L-BFGS and~HF into
Table~\ref{tab:first_order_results}.

\bibliographystyle{plainnat}
\clearpage
\bibliography{references/main,references/lecun}

\end{document}
